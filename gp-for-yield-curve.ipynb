{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9282ed7e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-26T09:18:23.608115Z",
     "iopub.status.busy": "2023-05-26T09:18:23.607033Z",
     "iopub.status.idle": "2023-05-26T09:18:40.835286Z",
     "shell.execute_reply": "2023-05-26T09:18:40.834063Z"
    },
    "papermill": {
     "duration": 17.238838,
     "end_time": "2023-05-26T09:18:40.837777",
     "exception": false,
     "start_time": "2023-05-26T09:18:23.598939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gpytorch\r\n",
      "  Downloading gpytorch-1.10-py3-none-any.whl (255 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from gpytorch) (1.2.2)\r\n",
      "Collecting linear-operator>=0.4.0 (from gpytorch)\r\n",
      "  Downloading linear_operator-0.4.0-py3-none-any.whl (156 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.7/156.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.10/site-packages (from linear-operator>=0.4.0->gpytorch) (2.0.0+cpu)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from linear-operator>=0.4.0->gpytorch) (1.10.1)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->gpytorch) (1.23.5)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->gpytorch) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->gpytorch) (3.1.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (3.12.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (4.5.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11->linear-operator>=0.4.0->gpytorch) (2.1.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11->linear-operator>=0.4.0->gpytorch) (1.3.0)\r\n",
      "Installing collected packages: linear-operator, gpytorch\r\n",
      "Successfully installed gpytorch-1.10 linear-operator-0.4.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import torch\n",
    "!pip install gpytorch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93f414e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T09:18:40.854513Z",
     "iopub.status.busy": "2023-05-26T09:18:40.853404Z",
     "iopub.status.idle": "2023-05-26T09:18:42.265545Z",
     "shell.execute_reply": "2023-05-26T09:18:42.264149Z"
    },
    "papermill": {
     "duration": 1.424033,
     "end_time": "2023-05-26T09:18:42.269018",
     "exception": false,
     "start_time": "2023-05-26T09:18:40.844985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "424a6c09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T09:18:42.287113Z",
     "iopub.status.busy": "2023-05-26T09:18:42.285741Z",
     "iopub.status.idle": "2023-05-26T09:18:42.381313Z",
     "shell.execute_reply": "2023-05-26T09:18:42.380020Z"
    },
    "papermill": {
     "duration": 0.108654,
     "end_time": "2023-05-26T09:18:42.385309",
     "exception": false,
     "start_time": "2023-05-26T09:18:42.276655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2002, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1 Mo</th>\n",
       "      <th>3 Mo</th>\n",
       "      <th>6 Mo</th>\n",
       "      <th>1 Yr</th>\n",
       "      <th>2 Yr</th>\n",
       "      <th>3 Yr</th>\n",
       "      <th>5 Yr</th>\n",
       "      <th>7 Yr</th>\n",
       "      <th>10 Yr</th>\n",
       "      <th>20 Yr</th>\n",
       "      <th>30 Yr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.66</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.36</td>\n",
       "      <td>3.85</td>\n",
       "      <td>4.60</td>\n",
       "      <td>4.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.56</td>\n",
       "      <td>3.28</td>\n",
       "      <td>3.77</td>\n",
       "      <td>4.54</td>\n",
       "      <td>4.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.33</td>\n",
       "      <td>3.85</td>\n",
       "      <td>4.63</td>\n",
       "      <td>4.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.62</td>\n",
       "      <td>2.62</td>\n",
       "      <td>3.33</td>\n",
       "      <td>3.85</td>\n",
       "      <td>4.62</td>\n",
       "      <td>4.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.56</td>\n",
       "      <td>2.57</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.83</td>\n",
       "      <td>4.61</td>\n",
       "      <td>4.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            1 Mo  3 Mo  6 Mo  1 Yr  2 Yr  3 Yr  5 Yr  7 Yr  10 Yr  20 Yr  \\\n",
       "Date                                                                       \n",
       "2010-01-04  0.05  0.08  0.18  0.45  1.09  1.66  2.65  3.36   3.85   4.60   \n",
       "2010-01-05  0.03  0.07  0.17  0.41  1.01  1.57  2.56  3.28   3.77   4.54   \n",
       "2010-01-06  0.03  0.06  0.15  0.40  1.01  1.60  2.60  3.33   3.85   4.63   \n",
       "2010-01-07  0.02  0.05  0.16  0.40  1.03  1.62  2.62  3.33   3.85   4.62   \n",
       "2010-01-08  0.02  0.05  0.15  0.37  0.96  1.56  2.57  3.31   3.83   4.61   \n",
       "\n",
       "            30 Yr  \n",
       "Date               \n",
       "2010-01-04   4.65  \n",
       "2010-01-05   4.59  \n",
       "2010-01-06   4.70  \n",
       "2010-01-07   4.69  \n",
       "2010-01-08   4.70  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = pd.read_csv(\"../input/us-treasury-yield-curve-2010-2018/yield_curve_2010_2018.csv\",index_col=0).drop(columns=\"Date.1\")\n",
    "df_clean.index = pd.DatetimeIndex(df_clean.index)\n",
    "df_clean = df_clean.sort_index()\n",
    "print (df_clean.shape)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39148f15",
   "metadata": {
    "papermill": {
     "duration": 0.007232,
     "end_time": "2023-05-26T09:18:42.402679",
     "exception": false,
     "start_time": "2023-05-26T09:18:42.395447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# EDA\n",
    "1. Hurst Component / Autocorelation\n",
    "2. Persistence Model $\\hat{Y}_{t+1} = Y_{t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f662932",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T09:18:42.420034Z",
     "iopub.status.busy": "2023-05-26T09:18:42.419602Z",
     "iopub.status.idle": "2023-05-26T09:18:42.427414Z",
     "shell.execute_reply": "2023-05-26T09:18:42.425578Z"
    },
    "papermill": {
     "duration": 0.019697,
     "end_time": "2023-05-26T09:18:42.430006",
     "exception": false,
     "start_time": "2023-05-26T09:18:42.410309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_hurst_exponent(time_series, max_lag=20):\n",
    "    \"\"\"Returns the Hurst Exponent of the time series\"\"\"\n",
    "    lags = range(2, max_lag)\n",
    "    # variances of the lagged differences\n",
    "    tau = [np.std(np.subtract(time_series[lag:], time_series[:-lag])) for lag in lags]\n",
    "    # calculate the slope of the log plot -> the Hurst Exponent\n",
    "    reg = np.polyfit(np.log(lags), np.log(tau), 1)\n",
    "    return reg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "637f8216",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T09:18:42.447830Z",
     "iopub.status.busy": "2023-05-26T09:18:42.446943Z",
     "iopub.status.idle": "2023-05-26T09:18:42.493139Z",
     "shell.execute_reply": "2023-05-26T09:18:42.491990Z"
    },
    "papermill": {
     "duration": 0.05785,
     "end_time": "2023-05-26T09:18:42.495664",
     "exception": false,
     "start_time": "2023-05-26T09:18:42.437814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1 Mo</th>\n",
       "      <th>3 Mo</th>\n",
       "      <th>6 Mo</th>\n",
       "      <th>1 Yr</th>\n",
       "      <th>2 Yr</th>\n",
       "      <th>3 Yr</th>\n",
       "      <th>5 Yr</th>\n",
       "      <th>7 Yr</th>\n",
       "      <th>10 Yr</th>\n",
       "      <th>20 Yr</th>\n",
       "      <th>30 Yr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.33008</td>\n",
       "      <td>0.460147</td>\n",
       "      <td>0.524739</td>\n",
       "      <td>0.498605</td>\n",
       "      <td>0.443841</td>\n",
       "      <td>0.469158</td>\n",
       "      <td>0.489996</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.493751</td>\n",
       "      <td>0.489162</td>\n",
       "      <td>0.470324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      1 Mo      3 Mo      6 Mo      1 Yr      2 Yr      3 Yr      5 Yr  \\\n",
       "0  0.33008  0.460147  0.524739  0.498605  0.443841  0.469158  0.489996   \n",
       "\n",
       "       7 Yr     10 Yr     20 Yr     30 Yr  \n",
       "0  0.490909  0.493751  0.489162  0.470324  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_clean.apply(lambda x: get_hurst_exponent(x.values, 40))).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d057783",
   "metadata": {
    "papermill": {
     "duration": 0.007307,
     "end_time": "2023-05-26T09:18:42.510864",
     "exception": false,
     "start_time": "2023-05-26T09:18:42.503557",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Train/Validation/Test\n",
    "Train + Validation = 2010 to 2017  \n",
    "Test = 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b3bd09a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T09:18:42.528726Z",
     "iopub.status.busy": "2023-05-26T09:18:42.527595Z",
     "iopub.status.idle": "2023-05-26T09:18:42.728267Z",
     "shell.execute_reply": "2023-05-26T09:18:42.726839Z"
    },
    "papermill": {
     "duration": 0.212605,
     "end_time": "2023-05-26T09:18:42.731141",
     "exception": false,
     "start_time": "2023-05-26T09:18:42.518536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3c701_row0_col0, #T_3c701_row1_col1, #T_3c701_row2_col2, #T_3c701_row3_col3, #T_3c701_row4_col4, #T_3c701_row5_col5, #T_3c701_row6_col6, #T_3c701_row7_col7, #T_3c701_row8_col8, #T_3c701_row9_col9, #T_3c701_row9_col10, #T_3c701_row10_col9, #T_3c701_row10_col10 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row0_col1, #T_3c701_row1_col0, #T_3c701_row1_col2 {\n",
       "  background-color: #02395a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row0_col2, #T_3c701_row2_col0 {\n",
       "  background-color: #023c5f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row0_col3, #T_3c701_row3_col0, #T_3c701_row8_col9 {\n",
       "  background-color: #034165;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row0_col4 {\n",
       "  background-color: #045c90;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row0_col5 {\n",
       "  background-color: #3d93c2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row0_col6, #T_3c701_row0_col7, #T_3c701_row2_col7, #T_3c701_row2_col8, #T_3c701_row2_col9, #T_3c701_row2_col10, #T_3c701_row10_col0, #T_3c701_row10_col1, #T_3c701_row10_col2, #T_3c701_row10_col3, #T_3c701_row10_col4, #T_3c701_row10_col5 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row0_col8, #T_3c701_row2_col6 {\n",
       "  background-color: #fbf4f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row0_col9, #T_3c701_row3_col8, #T_3c701_row9_col0, #T_3c701_row9_col1, #T_3c701_row9_col3 {\n",
       "  background-color: #faf3f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row0_col10, #T_3c701_row1_col9, #T_3c701_row1_col10, #T_3c701_row9_col2 {\n",
       "  background-color: #fbf3f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row1_col3, #T_3c701_row3_col1 {\n",
       "  background-color: #023e62;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row1_col4 {\n",
       "  background-color: #045a8d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row1_col5 {\n",
       "  background-color: #358fc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row1_col6, #T_3c701_row3_col10 {\n",
       "  background-color: #fef6fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row1_col7 {\n",
       "  background-color: #fef6fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row1_col8 {\n",
       "  background-color: #fcf4fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row2_col1, #T_3c701_row2_col3, #T_3c701_row3_col2 {\n",
       "  background-color: #023a5b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row2_col4 {\n",
       "  background-color: #045382;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row2_col5, #T_3c701_row5_col7 {\n",
       "  background-color: #2987bc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row3_col4, #T_3c701_row10_col8 {\n",
       "  background-color: #034871;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row3_col5, #T_3c701_row5_col0 {\n",
       "  background-color: #0c74b2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row3_col6 {\n",
       "  background-color: #ede7f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row3_col7 {\n",
       "  background-color: #f5eff6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row3_col9 {\n",
       "  background-color: #fdf5fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row4_col0 {\n",
       "  background-color: #04598c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row4_col1 {\n",
       "  background-color: #045687;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row4_col2 {\n",
       "  background-color: #034e7b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row4_col3, #T_3c701_row8_col10 {\n",
       "  background-color: #03466e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row4_col5, #T_3c701_row8_col7 {\n",
       "  background-color: #034a74;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row4_col6 {\n",
       "  background-color: #6ba5cd;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row4_col7 {\n",
       "  background-color: #a5bddb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row4_col8 {\n",
       "  background-color: #ced0e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row4_col9 {\n",
       "  background-color: #e1dfed;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row4_col10 {\n",
       "  background-color: #e6e2ef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row5_col1 {\n",
       "  background-color: #0771b1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row5_col2 {\n",
       "  background-color: #056caa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row5_col3, #T_3c701_row5_col6 {\n",
       "  background-color: #04649d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row5_col4, #T_3c701_row6_col7, #T_3c701_row7_col8 {\n",
       "  background-color: #03476f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row5_col8 {\n",
       "  background-color: #78abd0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row5_col9 {\n",
       "  background-color: #abbfdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row5_col10 {\n",
       "  background-color: #b5c4df;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row6_col0 {\n",
       "  background-color: #7bacd1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row6_col1 {\n",
       "  background-color: #79abd0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row6_col2 {\n",
       "  background-color: #6fa7ce;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row6_col3 {\n",
       "  background-color: #5a9ec9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row6_col4 {\n",
       "  background-color: #1379b5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row6_col5 {\n",
       "  background-color: #045b8f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row6_col8, #T_3c701_row7_col10 {\n",
       "  background-color: #04649e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row6_col9 {\n",
       "  background-color: #2081b9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row6_col10 {\n",
       "  background-color: #2d8abd;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row7_col0 {\n",
       "  background-color: #bbc7e0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row7_col1 {\n",
       "  background-color: #b8c6e0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row7_col2 {\n",
       "  background-color: #b4c4df;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row7_col3 {\n",
       "  background-color: #a7bddb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row7_col4 {\n",
       "  background-color: #6da6cd;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row7_col5 {\n",
       "  background-color: #2786bb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row7_col6 {\n",
       "  background-color: #034b76;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row7_col9 {\n",
       "  background-color: #045f95;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row8_col0, #T_3c701_row8_col1 {\n",
       "  background-color: #e5e1ef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row8_col2 {\n",
       "  background-color: #e4e1ef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row8_col3 {\n",
       "  background-color: #e0dded;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row8_col4 {\n",
       "  background-color: #cccfe5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row8_col5 {\n",
       "  background-color: #a9bfdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row8_col6 {\n",
       "  background-color: #1c7fb8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row9_col4 {\n",
       "  background-color: #f7f0f7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row9_col5 {\n",
       "  background-color: #f4edf6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row9_col6 {\n",
       "  background-color: #a2bcda;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row9_col7 {\n",
       "  background-color: #056ba9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row9_col8 {\n",
       "  background-color: #034267;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3c701_row10_col6 {\n",
       "  background-color: #c1cae2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3c701_row10_col7 {\n",
       "  background-color: #0f76b3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3c701\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3c701_level0_col0\" class=\"col_heading level0 col0\" >1 Mo</th>\n",
       "      <th id=\"T_3c701_level0_col1\" class=\"col_heading level0 col1\" >3 Mo</th>\n",
       "      <th id=\"T_3c701_level0_col2\" class=\"col_heading level0 col2\" >6 Mo</th>\n",
       "      <th id=\"T_3c701_level0_col3\" class=\"col_heading level0 col3\" >1 Yr</th>\n",
       "      <th id=\"T_3c701_level0_col4\" class=\"col_heading level0 col4\" >2 Yr</th>\n",
       "      <th id=\"T_3c701_level0_col5\" class=\"col_heading level0 col5\" >3 Yr</th>\n",
       "      <th id=\"T_3c701_level0_col6\" class=\"col_heading level0 col6\" >5 Yr</th>\n",
       "      <th id=\"T_3c701_level0_col7\" class=\"col_heading level0 col7\" >7 Yr</th>\n",
       "      <th id=\"T_3c701_level0_col8\" class=\"col_heading level0 col8\" >10 Yr</th>\n",
       "      <th id=\"T_3c701_level0_col9\" class=\"col_heading level0 col9\" >20 Yr</th>\n",
       "      <th id=\"T_3c701_level0_col10\" class=\"col_heading level0 col10\" >30 Yr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3c701_level0_row0\" class=\"row_heading level0 row0\" >1 Mo</th>\n",
       "      <td id=\"T_3c701_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_3c701_row0_col1\" class=\"data row0 col1\" >0.992347</td>\n",
       "      <td id=\"T_3c701_row0_col2\" class=\"data row0 col2\" >0.978688</td>\n",
       "      <td id=\"T_3c701_row0_col3\" class=\"data row0 col3\" >0.955501</td>\n",
       "      <td id=\"T_3c701_row0_col4\" class=\"data row0 col4\" >0.843656</td>\n",
       "      <td id=\"T_3c701_row0_col5\" class=\"data row0 col5\" >0.658935</td>\n",
       "      <td id=\"T_3c701_row0_col6\" class=\"data row0 col6\" >0.337214</td>\n",
       "      <td id=\"T_3c701_row0_col7\" class=\"data row0 col7\" >0.125799</td>\n",
       "      <td id=\"T_3c701_row0_col8\" class=\"data row0 col8\" >-0.076583</td>\n",
       "      <td id=\"T_3c701_row0_col9\" class=\"data row0 col9\" >-0.235160</td>\n",
       "      <td id=\"T_3c701_row0_col10\" class=\"data row0 col10\" >-0.277069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c701_level0_row1\" class=\"row_heading level0 row1\" >3 Mo</th>\n",
       "      <td id=\"T_3c701_row1_col0\" class=\"data row1 col0\" >0.992347</td>\n",
       "      <td id=\"T_3c701_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "      <td id=\"T_3c701_row1_col2\" class=\"data row1 col2\" >0.989907</td>\n",
       "      <td id=\"T_3c701_row1_col3\" class=\"data row1 col3\" >0.968589</td>\n",
       "      <td id=\"T_3c701_row1_col4\" class=\"data row1 col4\" >0.857941</td>\n",
       "      <td id=\"T_3c701_row1_col5\" class=\"data row1 col5\" >0.671222</td>\n",
       "      <td id=\"T_3c701_row1_col6\" class=\"data row1 col6\" >0.344163</td>\n",
       "      <td id=\"T_3c701_row1_col7\" class=\"data row1 col7\" >0.129381</td>\n",
       "      <td id=\"T_3c701_row1_col8\" class=\"data row1 col8\" >-0.077992</td>\n",
       "      <td id=\"T_3c701_row1_col9\" class=\"data row1 col9\" >-0.239536</td>\n",
       "      <td id=\"T_3c701_row1_col10\" class=\"data row1 col10\" >-0.281459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c701_level0_row2\" class=\"row_heading level0 row2\" >6 Mo</th>\n",
       "      <td id=\"T_3c701_row2_col0\" class=\"data row2 col0\" >0.978688</td>\n",
       "      <td id=\"T_3c701_row2_col1\" class=\"data row2 col1\" >0.989907</td>\n",
       "      <td id=\"T_3c701_row2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
       "      <td id=\"T_3c701_row2_col3\" class=\"data row2 col3\" >0.989115</td>\n",
       "      <td id=\"T_3c701_row2_col4\" class=\"data row2 col4\" >0.889561</td>\n",
       "      <td id=\"T_3c701_row2_col5\" class=\"data row2 col5\" >0.700382</td>\n",
       "      <td id=\"T_3c701_row2_col6\" class=\"data row2 col6\" >0.353527</td>\n",
       "      <td id=\"T_3c701_row2_col7\" class=\"data row2 col7\" >0.124277</td>\n",
       "      <td id=\"T_3c701_row2_col8\" class=\"data row2 col8\" >-0.103065</td>\n",
       "      <td id=\"T_3c701_row2_col9\" class=\"data row2 col9\" >-0.277396</td>\n",
       "      <td id=\"T_3c701_row2_col10\" class=\"data row2 col10\" >-0.318028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c701_level0_row3\" class=\"row_heading level0 row3\" >1 Yr</th>\n",
       "      <td id=\"T_3c701_row3_col0\" class=\"data row3 col0\" >0.955501</td>\n",
       "      <td id=\"T_3c701_row3_col1\" class=\"data row3 col1\" >0.968589</td>\n",
       "      <td id=\"T_3c701_row3_col2\" class=\"data row3 col2\" >0.989115</td>\n",
       "      <td id=\"T_3c701_row3_col3\" class=\"data row3 col3\" >1.000000</td>\n",
       "      <td id=\"T_3c701_row3_col4\" class=\"data row3 col4\" >0.933835</td>\n",
       "      <td id=\"T_3c701_row3_col5\" class=\"data row3 col5\" >0.762874</td>\n",
       "      <td id=\"T_3c701_row3_col6\" class=\"data row3 col6\" >0.418554</td>\n",
       "      <td id=\"T_3c701_row3_col7\" class=\"data row3 col7\" >0.181262</td>\n",
       "      <td id=\"T_3c701_row3_col8\" class=\"data row3 col8\" >-0.066014</td>\n",
       "      <td id=\"T_3c701_row3_col9\" class=\"data row3 col9\" >-0.258921</td>\n",
       "      <td id=\"T_3c701_row3_col10\" class=\"data row3 col10\" >-0.302840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c701_level0_row4\" class=\"row_heading level0 row4\" >2 Yr</th>\n",
       "      <td id=\"T_3c701_row4_col0\" class=\"data row4 col0\" >0.843656</td>\n",
       "      <td id=\"T_3c701_row4_col1\" class=\"data row4 col1\" >0.857941</td>\n",
       "      <td id=\"T_3c701_row4_col2\" class=\"data row4 col2\" >0.889561</td>\n",
       "      <td id=\"T_3c701_row4_col3\" class=\"data row4 col3\" >0.933835</td>\n",
       "      <td id=\"T_3c701_row4_col4\" class=\"data row4 col4\" >1.000000</td>\n",
       "      <td id=\"T_3c701_row4_col5\" class=\"data row4 col5\" >0.938132</td>\n",
       "      <td id=\"T_3c701_row4_col6\" class=\"data row4 col6\" >0.681302</td>\n",
       "      <td id=\"T_3c701_row4_col7\" class=\"data row4 col7\" >0.456014</td>\n",
       "      <td id=\"T_3c701_row4_col8\" class=\"data row4 col8\" >0.180907</td>\n",
       "      <td id=\"T_3c701_row4_col9\" class=\"data row4 col9\" >-0.057811</td>\n",
       "      <td id=\"T_3c701_row4_col10\" class=\"data row4 col10\" >-0.114843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c701_level0_row5\" class=\"row_heading level0 row5\" >3 Yr</th>\n",
       "      <td id=\"T_3c701_row5_col0\" class=\"data row5 col0\" >0.658935</td>\n",
       "      <td id=\"T_3c701_row5_col1\" class=\"data row5 col1\" >0.671222</td>\n",
       "      <td id=\"T_3c701_row5_col2\" class=\"data row5 col2\" >0.700382</td>\n",
       "      <td id=\"T_3c701_row5_col3\" class=\"data row5 col3\" >0.762874</td>\n",
       "      <td id=\"T_3c701_row5_col4\" class=\"data row5 col4\" >0.938132</td>\n",
       "      <td id=\"T_3c701_row5_col5\" class=\"data row5 col5\" >1.000000</td>\n",
       "      <td id=\"T_3c701_row5_col6\" class=\"data row5 col6\" >0.880052</td>\n",
       "      <td id=\"T_3c701_row5_col7\" class=\"data row5 col7\" >0.702053</td>\n",
       "      <td id=\"T_3c701_row5_col8\" class=\"data row5 col8\" >0.439560</td>\n",
       "      <td id=\"T_3c701_row5_col9\" class=\"data row5 col9\" >0.184803</td>\n",
       "      <td id=\"T_3c701_row5_col10\" class=\"data row5 col10\" >0.119278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c701_level0_row6\" class=\"row_heading level0 row6\" >5 Yr</th>\n",
       "      <td id=\"T_3c701_row6_col0\" class=\"data row6 col0\" >0.337214</td>\n",
       "      <td id=\"T_3c701_row6_col1\" class=\"data row6 col1\" >0.344163</td>\n",
       "      <td id=\"T_3c701_row6_col2\" class=\"data row6 col2\" >0.353527</td>\n",
       "      <td id=\"T_3c701_row6_col3\" class=\"data row6 col3\" >0.418554</td>\n",
       "      <td id=\"T_3c701_row6_col4\" class=\"data row6 col4\" >0.681302</td>\n",
       "      <td id=\"T_3c701_row6_col5\" class=\"data row6 col5\" >0.880052</td>\n",
       "      <td id=\"T_3c701_row6_col6\" class=\"data row6 col6\" >1.000000</td>\n",
       "      <td id=\"T_3c701_row6_col7\" class=\"data row6 col7\" >0.952005</td>\n",
       "      <td id=\"T_3c701_row6_col8\" class=\"data row6 col8\" >0.794160</td>\n",
       "      <td id=\"T_3c701_row6_col9\" class=\"data row6 col9\" >0.591062</td>\n",
       "      <td id=\"T_3c701_row6_col10\" class=\"data row6 col10\" >0.531850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c701_level0_row7\" class=\"row_heading level0 row7\" >7 Yr</th>\n",
       "      <td id=\"T_3c701_row7_col0\" class=\"data row7 col0\" >0.125799</td>\n",
       "      <td id=\"T_3c701_row7_col1\" class=\"data row7 col1\" >0.129381</td>\n",
       "      <td id=\"T_3c701_row7_col2\" class=\"data row7 col2\" >0.124277</td>\n",
       "      <td id=\"T_3c701_row7_col3\" class=\"data row7 col3\" >0.181262</td>\n",
       "      <td id=\"T_3c701_row7_col4\" class=\"data row7 col4\" >0.456014</td>\n",
       "      <td id=\"T_3c701_row7_col5\" class=\"data row7 col5\" >0.702053</td>\n",
       "      <td id=\"T_3c701_row7_col6\" class=\"data row7 col6\" >0.952005</td>\n",
       "      <td id=\"T_3c701_row7_col7\" class=\"data row7 col7\" >1.000000</td>\n",
       "      <td id=\"T_3c701_row7_col8\" class=\"data row7 col8\" >0.939376</td>\n",
       "      <td id=\"T_3c701_row7_col9\" class=\"data row7 col9\" >0.803195</td>\n",
       "      <td id=\"T_3c701_row7_col10\" class=\"data row7 col10\" >0.757528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c701_level0_row8\" class=\"row_heading level0 row8\" >10 Yr</th>\n",
       "      <td id=\"T_3c701_row8_col0\" class=\"data row8 col0\" >-0.076583</td>\n",
       "      <td id=\"T_3c701_row8_col1\" class=\"data row8 col1\" >-0.077992</td>\n",
       "      <td id=\"T_3c701_row8_col2\" class=\"data row8 col2\" >-0.103065</td>\n",
       "      <td id=\"T_3c701_row8_col3\" class=\"data row8 col3\" >-0.066014</td>\n",
       "      <td id=\"T_3c701_row8_col4\" class=\"data row8 col4\" >0.180907</td>\n",
       "      <td id=\"T_3c701_row8_col5\" class=\"data row8 col5\" >0.439560</td>\n",
       "      <td id=\"T_3c701_row8_col6\" class=\"data row8 col6\" >0.794160</td>\n",
       "      <td id=\"T_3c701_row8_col7\" class=\"data row8 col7\" >0.939376</td>\n",
       "      <td id=\"T_3c701_row8_col8\" class=\"data row8 col8\" >1.000000</td>\n",
       "      <td id=\"T_3c701_row8_col9\" class=\"data row8 col9\" >0.957449</td>\n",
       "      <td id=\"T_3c701_row8_col10\" class=\"data row8 col10\" >0.932447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c701_level0_row9\" class=\"row_heading level0 row9\" >20 Yr</th>\n",
       "      <td id=\"T_3c701_row9_col0\" class=\"data row9 col0\" >-0.235160</td>\n",
       "      <td id=\"T_3c701_row9_col1\" class=\"data row9 col1\" >-0.239536</td>\n",
       "      <td id=\"T_3c701_row9_col2\" class=\"data row9 col2\" >-0.277396</td>\n",
       "      <td id=\"T_3c701_row9_col3\" class=\"data row9 col3\" >-0.258921</td>\n",
       "      <td id=\"T_3c701_row9_col4\" class=\"data row9 col4\" >-0.057811</td>\n",
       "      <td id=\"T_3c701_row9_col5\" class=\"data row9 col5\" >0.184803</td>\n",
       "      <td id=\"T_3c701_row9_col6\" class=\"data row9 col6\" >0.591062</td>\n",
       "      <td id=\"T_3c701_row9_col7\" class=\"data row9 col7\" >0.803195</td>\n",
       "      <td id=\"T_3c701_row9_col8\" class=\"data row9 col8\" >0.957449</td>\n",
       "      <td id=\"T_3c701_row9_col9\" class=\"data row9 col9\" >1.000000</td>\n",
       "      <td id=\"T_3c701_row9_col10\" class=\"data row9 col10\" >0.995390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c701_level0_row10\" class=\"row_heading level0 row10\" >30 Yr</th>\n",
       "      <td id=\"T_3c701_row10_col0\" class=\"data row10 col0\" >-0.277069</td>\n",
       "      <td id=\"T_3c701_row10_col1\" class=\"data row10 col1\" >-0.281459</td>\n",
       "      <td id=\"T_3c701_row10_col2\" class=\"data row10 col2\" >-0.318028</td>\n",
       "      <td id=\"T_3c701_row10_col3\" class=\"data row10 col3\" >-0.302840</td>\n",
       "      <td id=\"T_3c701_row10_col4\" class=\"data row10 col4\" >-0.114843</td>\n",
       "      <td id=\"T_3c701_row10_col5\" class=\"data row10 col5\" >0.119278</td>\n",
       "      <td id=\"T_3c701_row10_col6\" class=\"data row10 col6\" >0.531850</td>\n",
       "      <td id=\"T_3c701_row10_col7\" class=\"data row10 col7\" >0.757528</td>\n",
       "      <td id=\"T_3c701_row10_col8\" class=\"data row10 col8\" >0.932447</td>\n",
       "      <td id=\"T_3c701_row10_col9\" class=\"data row10 col9\" >0.995390</td>\n",
       "      <td id=\"T_3c701_row10_col10\" class=\"data row10 col10\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7e623ed38850>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = df_clean.corr()\n",
    "corr.style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9619bdb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T09:18:42.750878Z",
     "iopub.status.busy": "2023-05-26T09:18:42.750398Z",
     "iopub.status.idle": "2023-05-26T09:18:42.770462Z",
     "shell.execute_reply": "2023-05-26T09:18:42.768778Z"
    },
    "papermill": {
     "duration": 0.033421,
     "end_time": "2023-05-26T09:18:42.773314",
     "exception": false,
     "start_time": "2023-05-26T09:18:42.739893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1752, 11), (250, 11))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [[\"1 Mo\",\"3 Mo\",\"2 Yr\", \"10 Yr\"]]\n",
    "# [[\"1 Mo\",\"3 Mo\", \"6 Mo\",\"1 Yr\" , \"2 Yr\", \"2 Yr\", \"5 Yr\" ,\"10 Yr\", \"20 Yr\" ,\"30 Yr\"]]\n",
    "\n",
    "train_validation = df_clean.loc[:\"2016\"]\n",
    "test = df_clean.loc[\"2017\":]\n",
    "\n",
    "Y_COLS = [\"2 Yr\", \"10 Yr\"]\n",
    "\n",
    "train_validation.shape,test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f1f761",
   "metadata": {
    "papermill": {
     "duration": 0.007982,
     "end_time": "2023-05-26T09:18:42.790050",
     "exception": false,
     "start_time": "2023-05-26T09:18:42.782068",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Persistence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37002b90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T09:18:42.808152Z",
     "iopub.status.busy": "2023-05-26T09:18:42.807762Z",
     "iopub.status.idle": "2023-05-26T09:18:42.838398Z",
     "shell.execute_reply": "2023-05-26T09:18:42.837310Z"
    },
    "papermill": {
     "duration": 0.042544,
     "end_time": "2023-05-26T09:18:42.840698",
     "exception": false,
     "start_time": "2023-05-26T09:18:42.798154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "Training from 2010-01-04 00:00:00 to 2014-01-02 00:00:00\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Fold 1:\n",
      "Training from 2010-01-04 00:00:00 to 2015-01-02 00:00:00\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Fold 2:\n",
      "Training from 2010-01-04 00:00:00 to 2015-12-31 00:00:00\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_persist = pd.DataFrame()\n",
    "tscv = TimeSeriesSplit(n_splits=3,test_size=250)\n",
    "for i, (train_index, test_index) in enumerate(tscv.split(train_validation)):\n",
    "    results_i = {}\n",
    "    print(f\"Fold {i}:\")\n",
    "    df_train_i = train_validation.iloc[train_index]\n",
    "    df_test_i = train_validation.iloc[test_index]\n",
    "    print(f\"Training from {df_train_i.index.min()} to {df_train_i.index.max()}\")\n",
    "    print(f\"Testing from {df_test_i.index.min()} to {df_test_i.index.max()}\\n\")\n",
    "    #persistence model with rolling one day prediction\n",
    "    # y_pred_i = pd.DataFrame([df_train_i.iloc[-1].values]*len(df_test_i),index=df_test_i.index,\\\n",
    "    #            columns = df_test_i.columns)\n",
    "    y_pred_i = df_test_i[Y_COLS].shift(1).copy()\n",
    "    y_pred_i.iloc[0] = df_train_i[Y_COLS].iloc[-1]\n",
    "    results_i = {\n",
    "      \"fold\":i,\n",
    "      \"test_year\":df_test_i.index.year.max()}\n",
    "    for test_col in Y_COLS:\n",
    "        results_i[test_col] = {\n",
    "          \"RMSE\": mean_squared_error(df_test_i[test_col],y_pred_i[test_col])**0.5,\\\n",
    "          \"MAPE\": mean_absolute_percentage_error(df_test_i[test_col],y_pred_i[test_col])    \n",
    "        }\n",
    "    result_persist = pd.concat([result_persist,pd.DataFrame.from_dict(results_i,orient=\"columns\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "415ca224",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T09:18:42.859054Z",
     "iopub.status.busy": "2023-05-26T09:18:42.858651Z",
     "iopub.status.idle": "2023-05-26T09:18:42.878000Z",
     "shell.execute_reply": "2023-05-26T09:18:42.876963Z"
    },
    "papermill": {
     "duration": 0.031938,
     "end_time": "2023-05-26T09:18:42.880737",
     "exception": false,
     "start_time": "2023-05-26T09:18:42.848799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>2 Yr</th>\n",
       "      <th>10 Yr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAPE</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.035398</td>\n",
       "      <td>0.017349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.030475</td>\n",
       "      <td>0.045728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fold      2 Yr     10 Yr\n",
       "index                          \n",
       "MAPE    1.0  0.035398  0.017349\n",
       "RMSE    1.0  0.030475  0.045728"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_persist.reset_index().drop(columns=[\"test_year\"]).groupby(\"index\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb3344",
   "metadata": {
    "papermill": {
     "duration": 0.008057,
     "end_time": "2023-05-26T09:18:42.897992",
     "exception": false,
     "start_time": "2023-05-26T09:18:42.889935",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Use Gaussian Process to model yield curve\n",
    "## Create lag variables for x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8971fb13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T09:18:42.917533Z",
     "iopub.status.busy": "2023-05-26T09:18:42.916853Z",
     "iopub.status.idle": "2023-05-26T09:18:43.034003Z",
     "shell.execute_reply": "2023-05-26T09:18:43.032980Z"
    },
    "papermill": {
     "duration": 0.13005,
     "end_time": "2023-05-26T09:18:43.036593",
     "exception": false,
     "start_time": "2023-05-26T09:18:42.906543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1738, 163), (1738, 2))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_lag_variables(df,y_cols,n_lags=2):\n",
    "    df1= df.copy()\n",
    "    col_list = df1.columns\n",
    "    for lag_i in range(1,n_lags+1):\n",
    "        df1[[f\"{col_i}_lag{lag_i}\" for col_i in col_list]] = df1[col_list].shift(lag_i)\n",
    "    df1 = df1.dropna()\n",
    "    X = df1.drop(columns=y_cols)\n",
    "    y = df1[y_cols]\n",
    "    return X,y\n",
    "\n",
    "X,y = create_lag_variables(train_validation,Y_COLS,14)\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d416f",
   "metadata": {
    "papermill": {
     "duration": 0.00878,
     "end_time": "2023-05-26T09:18:43.053949",
     "exception": false,
     "start_time": "2023-05-26T09:18:43.045169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Initialise GP model with variable kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "704c0154",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T09:18:43.072782Z",
     "iopub.status.busy": "2023-05-26T09:18:43.072122Z",
     "iopub.status.idle": "2023-05-26T09:18:43.087556Z",
     "shell.execute_reply": "2023-05-26T09:18:43.086568Z"
    },
    "papermill": {
     "duration": 0.027372,
     "end_time": "2023-05-26T09:18:43.089716",
     "exception": false,
     "start_time": "2023-05-26T09:18:43.062344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, kernel_func, likelihood):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(), num_tasks=train_y.shape[-1]\n",
    "        )\n",
    "        self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "            kernel_func, num_tasks=train_y.shape[-1], rank=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "def train_gp(train_x, train_y, kernel_func, N_ITER = 50,verbose=True):\n",
    "    likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=train_y.shape[-1])\n",
    "    if type(train_x) != torch.Tensor:\n",
    "        train_x_tensor = torch.tensor(train_x.values).float()\n",
    "        train_y_tensor = torch.tensor(train_y.values).float()\n",
    "    else:\n",
    "        train_x_tensor = train_x\n",
    "        train_y_tensor = train_y\n",
    "    model = MultitaskGPModel(train_x_tensor, train_y_tensor,kernel_func, likelihood)\n",
    "#     print (\"Initialised GP\")\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    print (\"Training GP\")\n",
    "    for i in range(N_ITER):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x_tensor)\n",
    "        loss = -mll(output, train_y_tensor)\n",
    "        loss.backward()\n",
    "        if verbose and ((i%10==0) or (i==N_ITER-1)):\n",
    "            print('Iter %d/%d - Loss: %.3f' % (i + 1, N_ITER, loss.item()))\n",
    "        optimizer.step()\n",
    "    return model,likelihood\n",
    "\n",
    "def predict_gp(test_x,likelihood,model):\n",
    "    # Set into eval mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    test_x_tensor = torch.tensor(test_x.values).float()\n",
    "    # Make predictions\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        predictions = likelihood(model(test_x_tensor))\n",
    "        mean = predictions.mean\n",
    "        # lower, upper = predictions.confidence_region()   \n",
    "    return mean,predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be42fa8",
   "metadata": {
    "papermill": {
     "duration": 0.008016,
     "end_time": "2023-05-26T09:18:43.106251",
     "exception": false,
     "start_time": "2023-05-26T09:18:43.098235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Train with diff kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5d38d36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T09:18:43.125177Z",
     "iopub.status.busy": "2023-05-26T09:18:43.124801Z",
     "iopub.status.idle": "2023-05-26T09:18:43.158113Z",
     "shell.execute_reply": "2023-05-26T09:18:43.156946Z"
    },
    "papermill": {
     "duration": 0.046078,
     "end_time": "2023-05-26T09:18:43.160842",
     "exception": false,
     "start_time": "2023-05-26T09:18:43.114764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernel_repo = {\n",
    "    #SE\n",
    "    \"kernel1\":gpytorch.kernels.RBFKernel(),\\\n",
    "    #SE*Lin\n",
    "    \"kernel2\":gpytorch.kernels.RBFKernel()*gpytorch.kernels.LinearKernel(),\\\n",
    "#     #Periodic*Lin\n",
    "#     \"kernel3\":gpytorch.kernels.PeriodicKernel()*gpytorch.kernels.LinearKernel(),\\\n",
    "#     #SE*Lin + RQ\n",
    "#     \"kernel4\":gpytorch.kernels.RBFKernel()*gpytorch.kernels.LinearKernel() + gpytorch.kernels.RQKernel(),\\\n",
    "#     #SE*Lin + RQ\n",
    "#     \"kernel5\":gpytorch.kernels.RBFKernel()*gpytorch.kernels.PeriodicKernel() + gpytorch.kernels.RQKernel()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a385290c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T09:18:43.179204Z",
     "iopub.status.busy": "2023-05-26T09:18:43.178844Z",
     "iopub.status.idle": "2023-05-26T09:18:43.188611Z",
     "shell.execute_reply": "2023-05-26T09:18:43.187524Z"
    },
    "papermill": {
     "duration": 0.021586,
     "end_time": "2023-05-26T09:18:43.190802",
     "exception": false,
     "start_time": "2023-05-26T09:18:43.169216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_kernel_cv_result_raw(kernel_j, N_TRIALS=10, N_TRAINING_ITER=50, verbose=False):\n",
    "    result_j = pd.DataFrame()\n",
    "    model_dict_j = {}\n",
    "    for trial in range(N_TRIALS):\n",
    "        for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "            print(f\"Sample {trial}, Fold {i}:\")\n",
    "            train_xi,train_yi = X.iloc[train_index],y.iloc[train_index]\n",
    "            test_xi,test_yi = X.iloc[test_index],y.iloc[test_index]\n",
    "#             if verbose:\n",
    "#                 print(f\"Training from {train_xi.index.min()} to {train_xi.index.max()}\")\n",
    "            model_i,lik_i = train_gp(train_xi,train_yi,kernel_j,N_TRAINING_ITER,verbose)\n",
    "            model_dict_j[trial,i,\"model\"]=model_i\n",
    "            if verbose:\n",
    "                print(f\"Testing from {test_xi.index.min()} to {test_xi.index.max()}\\n\")\n",
    "            mean_i,pred_i = predict_gp(test_xi,lik_i,model_i)\n",
    "            model_dict_j[trial,i,\"prediction\"]=pred_i\n",
    "            results_i = {\n",
    "              \"trial\":trial,\n",
    "              \"fold\":i,\n",
    "              \"test_year\":test_xi.index.year.max()}\n",
    "            y_pred_i = pd.DataFrame(mean_i,index=test_yi.index,columns = test_yi.columns)\n",
    "            for test_col in test_yi.columns:\n",
    "                results_i[test_col] = {\n",
    "                  \"RMSE\": mean_squared_error(test_yi[test_col],y_pred_i[test_col])**0.5,\\\n",
    "                  \"MAPE\": mean_absolute_percentage_error(test_yi[test_col],y_pred_i[test_col])    \n",
    "                }\n",
    "            result_j = pd.concat([result_j,pd.DataFrame.from_dict(results_i,orient=\"columns\")])\n",
    "    return (result_j,model_dict_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b637dd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T09:18:43.209063Z",
     "iopub.status.busy": "2023-05-26T09:18:43.208707Z",
     "iopub.status.idle": "2023-05-26T09:19:58.475798Z",
     "shell.execute_reply": "2023-05-26T09:19:58.474686Z"
    },
    "papermill": {
     "duration": 75.27903,
     "end_time": "2023-05-26T09:19:58.478241",
     "exception": false,
     "start_time": "2023-05-26T09:18:43.199211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0, Fold 0:\n",
      "Training GP\n",
      "Iter 1/10 - Loss: 1.396\n",
      "Iter 10/10 - Loss: 0.872\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 0, Fold 1:\n",
      "Training GP\n",
      "Iter 1/10 - Loss: 1.184\n",
      "Iter 10/10 - Loss: 0.783\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 0, Fold 2:\n",
      "Training GP\n",
      "Iter 1/10 - Loss: 1.159\n",
      "Iter 10/10 - Loss: 0.781\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n",
      "Sample 1, Fold 0:\n",
      "Training GP\n",
      "Iter 1/10 - Loss: 1.134\n",
      "Iter 10/10 - Loss: 0.763\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 1, Fold 1:\n",
      "Training GP\n",
      "Iter 1/10 - Loss: 1.123\n",
      "Iter 10/10 - Loss: 0.752\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 1, Fold 2:\n",
      "Training GP\n",
      "Iter 1/10 - Loss: 1.116\n",
      "Iter 10/10 - Loss: 0.750\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n",
      "Sample 2, Fold 0:\n",
      "Training GP\n",
      "Iter 1/10 - Loss: 1.115\n",
      "Iter 10/10 - Loss: 0.754\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 2, Fold 1:\n",
      "Training GP\n",
      "Iter 1/10 - Loss: 1.101\n",
      "Iter 10/10 - Loss: 0.744\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 2, Fold 2:\n",
      "Training GP\n",
      "Iter 1/10 - Loss: 1.100\n",
      "Iter 10/10 - Loss: 0.742\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>2 Yr</th>\n",
       "      <th>10 Yr</th>\n",
       "      <th>err_avg</th>\n",
       "      <th>rnk_</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial</th>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>RMSE</th>\n",
       "      <td>0.118492</td>\n",
       "      <td>0.071385</td>\n",
       "      <td>0.094939</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 2 Yr     10 Yr   err_avg  rnk_\n",
       "trial index                                    \n",
       "2     RMSE   0.118492  0.071385  0.094939   1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_test,model_test = get_kernel_cv_result_raw(gpytorch.kernels.RBFKernel(),3,10,True)\n",
    "results_trials = result_test.loc[\"RMSE\"].reset_index().drop(columns=[\"test_year\",\"fold\"]).groupby([\"trial\",\"index\"]).mean()\n",
    "results_trials[\"err_avg\"] = results_trials.mean(axis=1)\n",
    "results_trials[\"rnk_\"] = results_trials[\"err_avg\"].rank()\n",
    "results_trials.loc[results_trials.rnk_==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22e3c061",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T09:19:58.500429Z",
     "iopub.status.busy": "2023-05-26T09:19:58.500027Z",
     "iopub.status.idle": "2023-05-26T11:38:57.586512Z",
     "shell.execute_reply": "2023-05-26T11:38:57.585277Z"
    },
    "papermill": {
     "duration": 8339.100918,
     "end_time": "2023-05-26T11:38:57.589303",
     "exception": false,
     "start_time": "2023-05-26T09:19:58.488385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using kernel1\n",
      "Sample 0, Fold 0:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.342\n",
      "Iter 11/200 - Loss: 0.805\n",
      "Iter 21/200 - Loss: 0.295\n",
      "Iter 31/200 - Loss: -0.227\n",
      "Iter 41/200 - Loss: -0.741\n",
      "Iter 51/200 - Loss: -1.229\n",
      "Iter 61/200 - Loss: -1.669\n",
      "Iter 71/200 - Loss: -2.032\n",
      "Iter 81/200 - Loss: -2.297\n",
      "Iter 91/200 - Loss: -2.471\n",
      "Iter 101/200 - Loss: -2.573\n",
      "Iter 111/200 - Loss: -2.635\n",
      "Iter 121/200 - Loss: -2.674\n",
      "Iter 131/200 - Loss: -2.700\n",
      "Iter 141/200 - Loss: -2.718\n",
      "Iter 151/200 - Loss: -2.732\n",
      "Iter 161/200 - Loss: -2.744\n",
      "Iter 171/200 - Loss: -2.753\n",
      "Iter 181/200 - Loss: -2.760\n",
      "Iter 191/200 - Loss: -2.767\n",
      "Iter 200/200 - Loss: -2.772\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 0, Fold 1:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.102\n",
      "Iter 11/200 - Loss: 0.700\n",
      "Iter 21/200 - Loss: 0.240\n",
      "Iter 31/200 - Loss: -0.253\n",
      "Iter 41/200 - Loss: -0.755\n",
      "Iter 51/200 - Loss: -1.245\n",
      "Iter 61/200 - Loss: -1.705\n",
      "Iter 71/200 - Loss: -2.106\n",
      "Iter 81/200 - Loss: -2.415\n",
      "Iter 91/200 - Loss: -2.615\n",
      "Iter 101/200 - Loss: -2.727\n",
      "Iter 111/200 - Loss: -2.785\n",
      "Iter 121/200 - Loss: -2.818\n",
      "Iter 131/200 - Loss: -2.838\n",
      "Iter 141/200 - Loss: -2.851\n",
      "Iter 151/200 - Loss: -2.860\n",
      "Iter 161/200 - Loss: -2.867\n",
      "Iter 171/200 - Loss: -2.873\n",
      "Iter 181/200 - Loss: -2.877\n",
      "Iter 191/200 - Loss: -2.881\n",
      "Iter 200/200 - Loss: -2.883\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 0, Fold 2:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.096\n",
      "Iter 11/200 - Loss: 0.696\n",
      "Iter 21/200 - Loss: 0.237\n",
      "Iter 31/200 - Loss: -0.258\n",
      "Iter 41/200 - Loss: -0.763\n",
      "Iter 51/200 - Loss: -1.258\n",
      "Iter 61/200 - Loss: -1.724\n",
      "Iter 71/200 - Loss: -2.133\n",
      "Iter 81/200 - Loss: -2.449\n",
      "Iter 91/200 - Loss: -2.651\n",
      "Iter 101/200 - Loss: -2.761\n",
      "Iter 111/200 - Loss: -2.818\n",
      "Iter 121/200 - Loss: -2.848\n",
      "Iter 131/200 - Loss: -2.867\n",
      "Iter 141/200 - Loss: -2.878\n",
      "Iter 151/200 - Loss: -2.887\n",
      "Iter 161/200 - Loss: -2.893\n",
      "Iter 171/200 - Loss: -2.897\n",
      "Iter 181/200 - Loss: -2.901\n",
      "Iter 191/200 - Loss: -2.904\n",
      "Iter 200/200 - Loss: -2.906\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n",
      "Sample 1, Fold 0:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.098\n",
      "Iter 11/200 - Loss: 0.698\n",
      "Iter 21/200 - Loss: 0.240\n",
      "Iter 31/200 - Loss: -0.253\n",
      "Iter 41/200 - Loss: -0.755\n",
      "Iter 51/200 - Loss: -1.244\n",
      "Iter 61/200 - Loss: -1.702\n",
      "Iter 71/200 - Loss: -2.101\n",
      "Iter 81/200 - Loss: -2.406\n",
      "Iter 91/200 - Loss: -2.602\n",
      "Iter 101/200 - Loss: -2.708\n",
      "Iter 111/200 - Loss: -2.764\n",
      "Iter 121/200 - Loss: -2.794\n",
      "Iter 131/200 - Loss: -2.811\n",
      "Iter 141/200 - Loss: -2.823\n",
      "Iter 151/200 - Loss: -2.831\n",
      "Iter 161/200 - Loss: -2.837\n",
      "Iter 171/200 - Loss: -2.841\n",
      "Iter 181/200 - Loss: -2.845\n",
      "Iter 191/200 - Loss: -2.848\n",
      "Iter 200/200 - Loss: -2.850\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 1, Fold 1:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.096\n",
      "Iter 11/200 - Loss: 0.697\n",
      "Iter 21/200 - Loss: 0.239\n",
      "Iter 31/200 - Loss: -0.255\n",
      "Iter 41/200 - Loss: -0.760\n",
      "Iter 51/200 - Loss: -1.254\n",
      "Iter 61/200 - Loss: -1.719\n",
      "Iter 71/200 - Loss: -2.126\n",
      "Iter 81/200 - Loss: -2.440\n",
      "Iter 91/200 - Loss: -2.641\n",
      "Iter 101/200 - Loss: -2.750\n",
      "Iter 111/200 - Loss: -2.806\n",
      "Iter 121/200 - Loss: -2.836\n",
      "Iter 131/200 - Loss: -2.854\n",
      "Iter 141/200 - Loss: -2.865\n",
      "Iter 151/200 - Loss: -2.873\n",
      "Iter 161/200 - Loss: -2.879\n",
      "Iter 171/200 - Loss: -2.883\n",
      "Iter 181/200 - Loss: -2.887\n",
      "Iter 191/200 - Loss: -2.890\n",
      "Iter 200/200 - Loss: -2.892\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 1, Fold 2:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.101\n",
      "Iter 11/200 - Loss: 0.698\n",
      "Iter 21/200 - Loss: 0.238\n",
      "Iter 31/200 - Loss: -0.258\n",
      "Iter 41/200 - Loss: -0.763\n",
      "Iter 51/200 - Loss: -1.260\n",
      "Iter 61/200 - Loss: -1.727\n",
      "Iter 71/200 - Loss: -2.138\n",
      "Iter 81/200 - Loss: -2.454\n",
      "Iter 91/200 - Loss: -2.656\n",
      "Iter 101/200 - Loss: -2.765\n",
      "Iter 111/200 - Loss: -2.821\n",
      "Iter 121/200 - Loss: -2.851\n",
      "Iter 131/200 - Loss: -2.869\n",
      "Iter 141/200 - Loss: -2.880\n",
      "Iter 151/200 - Loss: -2.888\n",
      "Iter 161/200 - Loss: -2.894\n",
      "Iter 171/200 - Loss: -2.898\n",
      "Iter 181/200 - Loss: -2.902\n",
      "Iter 191/200 - Loss: -2.905\n",
      "Iter 200/200 - Loss: -2.907\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n",
      "Sample 2, Fold 0:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.096\n",
      "Iter 11/200 - Loss: 0.699\n",
      "Iter 21/200 - Loss: 0.241\n",
      "Iter 31/200 - Loss: -0.252\n",
      "Iter 41/200 - Loss: -0.755\n",
      "Iter 51/200 - Loss: -1.245\n",
      "Iter 61/200 - Loss: -1.702\n",
      "Iter 71/200 - Loss: -2.101\n",
      "Iter 81/200 - Loss: -2.407\n",
      "Iter 91/200 - Loss: -2.603\n",
      "Iter 101/200 - Loss: -2.709\n",
      "Iter 111/200 - Loss: -2.764\n",
      "Iter 121/200 - Loss: -2.794\n",
      "Iter 131/200 - Loss: -2.811\n",
      "Iter 141/200 - Loss: -2.823\n",
      "Iter 151/200 - Loss: -2.831\n",
      "Iter 161/200 - Loss: -2.836\n",
      "Iter 171/200 - Loss: -2.841\n",
      "Iter 181/200 - Loss: -2.845\n",
      "Iter 191/200 - Loss: -2.848\n",
      "Iter 200/200 - Loss: -2.850\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 2, Fold 1:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.096\n",
      "Iter 11/200 - Loss: 0.698\n",
      "Iter 21/200 - Loss: 0.239\n",
      "Iter 31/200 - Loss: -0.256\n",
      "Iter 41/200 - Loss: -0.760\n",
      "Iter 51/200 - Loss: -1.254\n",
      "Iter 61/200 - Loss: -1.719\n",
      "Iter 71/200 - Loss: -2.127\n",
      "Iter 81/200 - Loss: -2.441\n",
      "Iter 91/200 - Loss: -2.641\n",
      "Iter 101/200 - Loss: -2.750\n",
      "Iter 111/200 - Loss: -2.806\n",
      "Iter 121/200 - Loss: -2.836\n",
      "Iter 131/200 - Loss: -2.854\n",
      "Iter 141/200 - Loss: -2.865\n",
      "Iter 151/200 - Loss: -2.873\n",
      "Iter 161/200 - Loss: -2.879\n",
      "Iter 171/200 - Loss: -2.884\n",
      "Iter 181/200 - Loss: -2.887\n",
      "Iter 191/200 - Loss: -2.890\n",
      "Iter 200/200 - Loss: -2.893\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 2, Fold 2:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.097\n",
      "Iter 11/200 - Loss: 0.698\n",
      "Iter 21/200 - Loss: 0.238\n",
      "Iter 31/200 - Loss: -0.257\n",
      "Iter 41/200 - Loss: -0.762\n",
      "Iter 51/200 - Loss: -1.258\n",
      "Iter 61/200 - Loss: -1.726\n",
      "Iter 71/200 - Loss: -2.136\n",
      "Iter 81/200 - Loss: -2.453\n",
      "Iter 91/200 - Loss: -2.655\n",
      "Iter 101/200 - Loss: -2.764\n",
      "Iter 111/200 - Loss: -2.821\n",
      "Iter 121/200 - Loss: -2.851\n",
      "Iter 131/200 - Loss: -2.868\n",
      "Iter 141/200 - Loss: -2.880\n",
      "Iter 151/200 - Loss: -2.888\n",
      "Iter 161/200 - Loss: -2.894\n",
      "Iter 171/200 - Loss: -2.898\n",
      "Iter 181/200 - Loss: -2.902\n",
      "Iter 191/200 - Loss: -2.905\n",
      "Iter 200/200 - Loss: -2.907\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n",
      "Sample 3, Fold 0:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.100\n",
      "Iter 11/200 - Loss: 0.699\n",
      "Iter 21/200 - Loss: 0.240\n",
      "Iter 31/200 - Loss: -0.253\n",
      "Iter 41/200 - Loss: -0.755\n",
      "Iter 51/200 - Loss: -1.245\n",
      "Iter 61/200 - Loss: -1.702\n",
      "Iter 71/200 - Loss: -2.101\n",
      "Iter 81/200 - Loss: -2.406\n",
      "Iter 91/200 - Loss: -2.602\n",
      "Iter 101/200 - Loss: -2.708\n",
      "Iter 111/200 - Loss: -2.763\n",
      "Iter 121/200 - Loss: -2.793\n",
      "Iter 131/200 - Loss: -2.811\n",
      "Iter 141/200 - Loss: -2.823\n",
      "Iter 151/200 - Loss: -2.830\n",
      "Iter 161/200 - Loss: -2.836\n",
      "Iter 171/200 - Loss: -2.841\n",
      "Iter 181/200 - Loss: -2.844\n",
      "Iter 191/200 - Loss: -2.847\n",
      "Iter 200/200 - Loss: -2.850\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 3, Fold 1:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.099\n",
      "Iter 11/200 - Loss: 0.698\n",
      "Iter 21/200 - Loss: 0.240\n",
      "Iter 31/200 - Loss: -0.254\n",
      "Iter 41/200 - Loss: -0.759\n",
      "Iter 51/200 - Loss: -1.253\n",
      "Iter 61/200 - Loss: -1.718\n",
      "Iter 71/200 - Loss: -2.126\n",
      "Iter 81/200 - Loss: -2.440\n",
      "Iter 91/200 - Loss: -2.641\n",
      "Iter 101/200 - Loss: -2.749\n",
      "Iter 111/200 - Loss: -2.805\n",
      "Iter 121/200 - Loss: -2.836\n",
      "Iter 131/200 - Loss: -2.853\n",
      "Iter 141/200 - Loss: -2.865\n",
      "Iter 151/200 - Loss: -2.873\n",
      "Iter 161/200 - Loss: -2.879\n",
      "Iter 171/200 - Loss: -2.883\n",
      "Iter 181/200 - Loss: -2.887\n",
      "Iter 191/200 - Loss: -2.890\n",
      "Iter 200/200 - Loss: -2.892\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 3, Fold 2:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.097\n",
      "Iter 11/200 - Loss: 0.698\n",
      "Iter 21/200 - Loss: 0.238\n",
      "Iter 31/200 - Loss: -0.258\n",
      "Iter 41/200 - Loss: -0.764\n",
      "Iter 51/200 - Loss: -1.260\n",
      "Iter 61/200 - Loss: -1.727\n",
      "Iter 71/200 - Loss: -2.137\n",
      "Iter 81/200 - Loss: -2.453\n",
      "Iter 91/200 - Loss: -2.655\n",
      "Iter 101/200 - Loss: -2.764\n",
      "Iter 111/200 - Loss: -2.820\n",
      "Iter 121/200 - Loss: -2.850\n",
      "Iter 131/200 - Loss: -2.868\n",
      "Iter 141/200 - Loss: -2.880\n",
      "Iter 151/200 - Loss: -2.888\n",
      "Iter 161/200 - Loss: -2.894\n",
      "Iter 171/200 - Loss: -2.898\n",
      "Iter 181/200 - Loss: -2.902\n",
      "Iter 191/200 - Loss: -2.905\n",
      "Iter 200/200 - Loss: -2.907\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n",
      "Sample 4, Fold 0:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.099\n",
      "Iter 11/200 - Loss: 0.700\n",
      "Iter 21/200 - Loss: 0.241\n",
      "Iter 31/200 - Loss: -0.253\n",
      "Iter 41/200 - Loss: -0.755\n",
      "Iter 51/200 - Loss: -1.245\n",
      "Iter 61/200 - Loss: -1.703\n",
      "Iter 71/200 - Loss: -2.102\n",
      "Iter 81/200 - Loss: -2.408\n",
      "Iter 91/200 - Loss: -2.604\n",
      "Iter 101/200 - Loss: -2.710\n",
      "Iter 111/200 - Loss: -2.765\n",
      "Iter 121/200 - Loss: -2.794\n",
      "Iter 131/200 - Loss: -2.812\n",
      "Iter 141/200 - Loss: -2.823\n",
      "Iter 151/200 - Loss: -2.831\n",
      "Iter 161/200 - Loss: -2.837\n",
      "Iter 171/200 - Loss: -2.841\n",
      "Iter 181/200 - Loss: -2.845\n",
      "Iter 191/200 - Loss: -2.848\n",
      "Iter 200/200 - Loss: -2.850\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 4, Fold 1:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.105\n",
      "Iter 11/200 - Loss: 0.699\n",
      "Iter 21/200 - Loss: 0.240\n",
      "Iter 31/200 - Loss: -0.254\n",
      "Iter 41/200 - Loss: -0.759\n",
      "Iter 51/200 - Loss: -1.254\n",
      "Iter 61/200 - Loss: -1.720\n",
      "Iter 71/200 - Loss: -2.127\n",
      "Iter 81/200 - Loss: -2.441\n",
      "Iter 91/200 - Loss: -2.642\n",
      "Iter 101/200 - Loss: -2.750\n",
      "Iter 111/200 - Loss: -2.806\n",
      "Iter 121/200 - Loss: -2.836\n",
      "Iter 131/200 - Loss: -2.854\n",
      "Iter 141/200 - Loss: -2.865\n",
      "Iter 151/200 - Loss: -2.873\n",
      "Iter 161/200 - Loss: -2.879\n",
      "Iter 171/200 - Loss: -2.884\n",
      "Iter 181/200 - Loss: -2.887\n",
      "Iter 191/200 - Loss: -2.890\n",
      "Iter 200/200 - Loss: -2.892\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 4, Fold 2:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.095\n",
      "Iter 11/200 - Loss: 0.697\n",
      "Iter 21/200 - Loss: 0.237\n",
      "Iter 31/200 - Loss: -0.258\n",
      "Iter 41/200 - Loss: -0.764\n",
      "Iter 51/200 - Loss: -1.260\n",
      "Iter 61/200 - Loss: -1.727\n",
      "Iter 71/200 - Loss: -2.137\n",
      "Iter 81/200 - Loss: -2.453\n",
      "Iter 91/200 - Loss: -2.655\n",
      "Iter 101/200 - Loss: -2.765\n",
      "Iter 111/200 - Loss: -2.821\n",
      "Iter 121/200 - Loss: -2.851\n",
      "Iter 131/200 - Loss: -2.869\n",
      "Iter 141/200 - Loss: -2.880\n",
      "Iter 151/200 - Loss: -2.888\n",
      "Iter 161/200 - Loss: -2.894\n",
      "Iter 171/200 - Loss: -2.898\n",
      "Iter 181/200 - Loss: -2.902\n",
      "Iter 191/200 - Loss: -2.905\n",
      "Iter 200/200 - Loss: -2.907\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n",
      "Sample 5, Fold 0:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.096\n",
      "Iter 11/200 - Loss: 0.698\n",
      "Iter 21/200 - Loss: 0.242\n",
      "Iter 31/200 - Loss: -0.249\n",
      "Iter 41/200 - Loss: -0.751\n",
      "Iter 51/200 - Loss: -1.242\n",
      "Iter 61/200 - Loss: -1.700\n",
      "Iter 71/200 - Loss: -2.099\n",
      "Iter 81/200 - Loss: -2.405\n",
      "Iter 91/200 - Loss: -2.601\n",
      "Iter 101/200 - Loss: -2.708\n",
      "Iter 111/200 - Loss: -2.763\n",
      "Iter 121/200 - Loss: -2.793\n",
      "Iter 131/200 - Loss: -2.810\n",
      "Iter 141/200 - Loss: -2.822\n",
      "Iter 151/200 - Loss: -2.829\n",
      "Iter 161/200 - Loss: -2.836\n",
      "Iter 171/200 - Loss: -2.840\n",
      "Iter 181/200 - Loss: -2.844\n",
      "Iter 191/200 - Loss: -2.847\n",
      "Iter 200/200 - Loss: -2.849\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 5, Fold 1:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.095\n",
      "Iter 11/200 - Loss: 0.697\n",
      "Iter 21/200 - Loss: 0.238\n",
      "Iter 31/200 - Loss: -0.256\n",
      "Iter 41/200 - Loss: -0.760\n",
      "Iter 51/200 - Loss: -1.254\n",
      "Iter 61/200 - Loss: -1.719\n",
      "Iter 71/200 - Loss: -2.127\n",
      "Iter 81/200 - Loss: -2.440\n",
      "Iter 91/200 - Loss: -2.641\n",
      "Iter 101/200 - Loss: -2.750\n",
      "Iter 111/200 - Loss: -2.806\n",
      "Iter 121/200 - Loss: -2.836\n",
      "Iter 131/200 - Loss: -2.854\n",
      "Iter 141/200 - Loss: -2.865\n",
      "Iter 151/200 - Loss: -2.873\n",
      "Iter 161/200 - Loss: -2.879\n",
      "Iter 171/200 - Loss: -2.883\n",
      "Iter 181/200 - Loss: -2.888\n",
      "Iter 191/200 - Loss: -2.890\n",
      "Iter 200/200 - Loss: -2.893\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 5, Fold 2:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.097\n",
      "Iter 11/200 - Loss: 0.698\n",
      "Iter 21/200 - Loss: 0.238\n",
      "Iter 31/200 - Loss: -0.258\n",
      "Iter 41/200 - Loss: -0.764\n",
      "Iter 51/200 - Loss: -1.260\n",
      "Iter 61/200 - Loss: -1.727\n",
      "Iter 71/200 - Loss: -2.137\n",
      "Iter 81/200 - Loss: -2.453\n",
      "Iter 91/200 - Loss: -2.655\n",
      "Iter 101/200 - Loss: -2.764\n",
      "Iter 111/200 - Loss: -2.820\n",
      "Iter 121/200 - Loss: -2.851\n",
      "Iter 131/200 - Loss: -2.868\n",
      "Iter 141/200 - Loss: -2.880\n",
      "Iter 151/200 - Loss: -2.888\n",
      "Iter 161/200 - Loss: -2.893\n",
      "Iter 171/200 - Loss: -2.898\n",
      "Iter 181/200 - Loss: -2.902\n",
      "Iter 191/200 - Loss: -2.905\n",
      "Iter 200/200 - Loss: -2.907\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n",
      "Sample 6, Fold 0:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.099\n",
      "Iter 11/200 - Loss: 0.700\n",
      "Iter 21/200 - Loss: 0.241\n",
      "Iter 31/200 - Loss: -0.252\n",
      "Iter 41/200 - Loss: -0.755\n",
      "Iter 51/200 - Loss: -1.245\n",
      "Iter 61/200 - Loss: -1.703\n",
      "Iter 71/200 - Loss: -2.102\n",
      "Iter 81/200 - Loss: -2.408\n",
      "Iter 91/200 - Loss: -2.603\n",
      "Iter 101/200 - Loss: -2.709\n",
      "Iter 111/200 - Loss: -2.764\n",
      "Iter 121/200 - Loss: -2.794\n",
      "Iter 131/200 - Loss: -2.811\n",
      "Iter 141/200 - Loss: -2.823\n",
      "Iter 151/200 - Loss: -2.831\n",
      "Iter 161/200 - Loss: -2.837\n",
      "Iter 171/200 - Loss: -2.841\n",
      "Iter 181/200 - Loss: -2.845\n",
      "Iter 191/200 - Loss: -2.848\n",
      "Iter 200/200 - Loss: -2.850\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 6, Fold 1:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.115\n",
      "Iter 11/200 - Loss: 0.700\n",
      "Iter 21/200 - Loss: 0.240\n",
      "Iter 31/200 - Loss: -0.254\n",
      "Iter 41/200 - Loss: -0.758\n",
      "Iter 51/200 - Loss: -1.254\n",
      "Iter 61/200 - Loss: -1.720\n",
      "Iter 71/200 - Loss: -2.129\n",
      "Iter 81/200 - Loss: -2.442\n",
      "Iter 91/200 - Loss: -2.642\n",
      "Iter 101/200 - Loss: -2.751\n",
      "Iter 111/200 - Loss: -2.806\n",
      "Iter 121/200 - Loss: -2.836\n",
      "Iter 131/200 - Loss: -2.854\n",
      "Iter 141/200 - Loss: -2.865\n",
      "Iter 151/200 - Loss: -2.873\n",
      "Iter 161/200 - Loss: -2.879\n",
      "Iter 171/200 - Loss: -2.883\n",
      "Iter 181/200 - Loss: -2.887\n",
      "Iter 191/200 - Loss: -2.890\n",
      "Iter 200/200 - Loss: -2.892\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 6, Fold 2:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.100\n",
      "Iter 11/200 - Loss: 0.699\n",
      "Iter 21/200 - Loss: 0.238\n",
      "Iter 31/200 - Loss: -0.258\n",
      "Iter 41/200 - Loss: -0.763\n",
      "Iter 51/200 - Loss: -1.259\n",
      "Iter 61/200 - Loss: -1.726\n",
      "Iter 71/200 - Loss: -2.136\n",
      "Iter 81/200 - Loss: -2.452\n",
      "Iter 91/200 - Loss: -2.654\n",
      "Iter 101/200 - Loss: -2.764\n",
      "Iter 111/200 - Loss: -2.820\n",
      "Iter 121/200 - Loss: -2.850\n",
      "Iter 131/200 - Loss: -2.868\n",
      "Iter 141/200 - Loss: -2.879\n",
      "Iter 151/200 - Loss: -2.887\n",
      "Iter 161/200 - Loss: -2.893\n",
      "Iter 171/200 - Loss: -2.898\n",
      "Iter 181/200 - Loss: -2.901\n",
      "Iter 191/200 - Loss: -2.905\n",
      "Iter 200/200 - Loss: -2.907\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n",
      "Sample 7, Fold 0:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.102\n",
      "Iter 11/200 - Loss: 0.699\n",
      "Iter 21/200 - Loss: 0.241\n",
      "Iter 31/200 - Loss: -0.253\n",
      "Iter 41/200 - Loss: -0.755\n",
      "Iter 51/200 - Loss: -1.245\n",
      "Iter 61/200 - Loss: -1.703\n",
      "Iter 71/200 - Loss: -2.102\n",
      "Iter 81/200 - Loss: -2.408\n",
      "Iter 91/200 - Loss: -2.603\n",
      "Iter 101/200 - Loss: -2.709\n",
      "Iter 111/200 - Loss: -2.764\n",
      "Iter 121/200 - Loss: -2.794\n",
      "Iter 131/200 - Loss: -2.811\n",
      "Iter 141/200 - Loss: -2.823\n",
      "Iter 151/200 - Loss: -2.831\n",
      "Iter 161/200 - Loss: -2.837\n",
      "Iter 171/200 - Loss: -2.841\n",
      "Iter 181/200 - Loss: -2.845\n",
      "Iter 191/200 - Loss: -2.848\n",
      "Iter 200/200 - Loss: -2.850\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 7, Fold 1:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.102\n",
      "Iter 11/200 - Loss: 0.700\n",
      "Iter 21/200 - Loss: 0.240\n",
      "Iter 31/200 - Loss: -0.255\n",
      "Iter 41/200 - Loss: -0.760\n",
      "Iter 51/200 - Loss: -1.255\n",
      "Iter 61/200 - Loss: -1.720\n",
      "Iter 71/200 - Loss: -2.127\n",
      "Iter 81/200 - Loss: -2.441\n",
      "Iter 91/200 - Loss: -2.642\n",
      "Iter 101/200 - Loss: -2.750\n",
      "Iter 111/200 - Loss: -2.806\n",
      "Iter 121/200 - Loss: -2.836\n",
      "Iter 131/200 - Loss: -2.854\n",
      "Iter 141/200 - Loss: -2.866\n",
      "Iter 151/200 - Loss: -2.873\n",
      "Iter 161/200 - Loss: -2.879\n",
      "Iter 171/200 - Loss: -2.884\n",
      "Iter 181/200 - Loss: -2.887\n",
      "Iter 191/200 - Loss: -2.890\n",
      "Iter 200/200 - Loss: -2.892\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 7, Fold 2:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.096\n",
      "Iter 11/200 - Loss: 0.697\n",
      "Iter 21/200 - Loss: 0.237\n",
      "Iter 31/200 - Loss: -0.258\n",
      "Iter 41/200 - Loss: -0.764\n",
      "Iter 51/200 - Loss: -1.260\n",
      "Iter 61/200 - Loss: -1.727\n",
      "Iter 71/200 - Loss: -2.138\n",
      "Iter 81/200 - Loss: -2.454\n",
      "Iter 91/200 - Loss: -2.656\n",
      "Iter 101/200 - Loss: -2.765\n",
      "Iter 111/200 - Loss: -2.821\n",
      "Iter 121/200 - Loss: -2.851\n",
      "Iter 131/200 - Loss: -2.869\n",
      "Iter 141/200 - Loss: -2.880\n",
      "Iter 151/200 - Loss: -2.888\n",
      "Iter 161/200 - Loss: -2.894\n",
      "Iter 171/200 - Loss: -2.899\n",
      "Iter 181/200 - Loss: -2.902\n",
      "Iter 191/200 - Loss: -2.905\n",
      "Iter 200/200 - Loss: -2.908\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n",
      "Sample 8, Fold 0:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.103\n",
      "Iter 11/200 - Loss: 0.700\n",
      "Iter 21/200 - Loss: 0.241\n",
      "Iter 31/200 - Loss: -0.252\n",
      "Iter 41/200 - Loss: -0.755\n",
      "Iter 51/200 - Loss: -1.245\n",
      "Iter 61/200 - Loss: -1.703\n",
      "Iter 71/200 - Loss: -2.103\n",
      "Iter 81/200 - Loss: -2.408\n",
      "Iter 91/200 - Loss: -2.604\n",
      "Iter 101/200 - Loss: -2.710\n",
      "Iter 111/200 - Loss: -2.765\n",
      "Iter 121/200 - Loss: -2.794\n",
      "Iter 131/200 - Loss: -2.812\n",
      "Iter 141/200 - Loss: -2.823\n",
      "Iter 151/200 - Loss: -2.831\n",
      "Iter 161/200 - Loss: -2.837\n",
      "Iter 171/200 - Loss: -2.841\n",
      "Iter 181/200 - Loss: -2.845\n",
      "Iter 191/200 - Loss: -2.848\n",
      "Iter 200/200 - Loss: -2.850\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 8, Fold 1:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.098\n",
      "Iter 11/200 - Loss: 0.699\n",
      "Iter 21/200 - Loss: 0.240\n",
      "Iter 31/200 - Loss: -0.254\n",
      "Iter 41/200 - Loss: -0.759\n",
      "Iter 51/200 - Loss: -1.253\n",
      "Iter 61/200 - Loss: -1.718\n",
      "Iter 71/200 - Loss: -2.126\n",
      "Iter 81/200 - Loss: -2.440\n",
      "Iter 91/200 - Loss: -2.641\n",
      "Iter 101/200 - Loss: -2.750\n",
      "Iter 111/200 - Loss: -2.806\n",
      "Iter 121/200 - Loss: -2.836\n",
      "Iter 131/200 - Loss: -2.854\n",
      "Iter 141/200 - Loss: -2.865\n",
      "Iter 151/200 - Loss: -2.873\n",
      "Iter 161/200 - Loss: -2.879\n",
      "Iter 171/200 - Loss: -2.883\n",
      "Iter 181/200 - Loss: -2.887\n",
      "Iter 191/200 - Loss: -2.890\n",
      "Iter 200/200 - Loss: -2.892\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 8, Fold 2:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.094\n",
      "Iter 11/200 - Loss: 0.697\n",
      "Iter 21/200 - Loss: 0.237\n",
      "Iter 31/200 - Loss: -0.258\n",
      "Iter 41/200 - Loss: -0.764\n",
      "Iter 51/200 - Loss: -1.260\n",
      "Iter 61/200 - Loss: -1.727\n",
      "Iter 71/200 - Loss: -2.137\n",
      "Iter 81/200 - Loss: -2.453\n",
      "Iter 91/200 - Loss: -2.655\n",
      "Iter 101/200 - Loss: -2.764\n",
      "Iter 111/200 - Loss: -2.821\n",
      "Iter 121/200 - Loss: -2.851\n",
      "Iter 131/200 - Loss: -2.868\n",
      "Iter 141/200 - Loss: -2.880\n",
      "Iter 151/200 - Loss: -2.888\n",
      "Iter 161/200 - Loss: -2.894\n",
      "Iter 171/200 - Loss: -2.898\n",
      "Iter 181/200 - Loss: -2.902\n",
      "Iter 191/200 - Loss: -2.905\n",
      "Iter 200/200 - Loss: -2.907\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n",
      "Sample 9, Fold 0:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.095\n",
      "Iter 11/200 - Loss: 0.698\n",
      "Iter 21/200 - Loss: 0.241\n",
      "Iter 31/200 - Loss: -0.252\n",
      "Iter 41/200 - Loss: -0.754\n",
      "Iter 51/200 - Loss: -1.244\n",
      "Iter 61/200 - Loss: -1.702\n",
      "Iter 71/200 - Loss: -2.101\n",
      "Iter 81/200 - Loss: -2.407\n",
      "Iter 91/200 - Loss: -2.602\n",
      "Iter 101/200 - Loss: -2.709\n",
      "Iter 111/200 - Loss: -2.764\n",
      "Iter 121/200 - Loss: -2.794\n",
      "Iter 131/200 - Loss: -2.811\n",
      "Iter 141/200 - Loss: -2.823\n",
      "Iter 151/200 - Loss: -2.831\n",
      "Iter 161/200 - Loss: -2.836\n",
      "Iter 171/200 - Loss: -2.841\n",
      "Iter 181/200 - Loss: -2.845\n",
      "Iter 191/200 - Loss: -2.848\n",
      "Iter 200/200 - Loss: -2.850\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 9, Fold 1:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.095\n",
      "Iter 11/200 - Loss: 0.697\n",
      "Iter 21/200 - Loss: 0.239\n",
      "Iter 31/200 - Loss: -0.255\n",
      "Iter 41/200 - Loss: -0.760\n",
      "Iter 51/200 - Loss: -1.254\n",
      "Iter 61/200 - Loss: -1.719\n",
      "Iter 71/200 - Loss: -2.126\n",
      "Iter 81/200 - Loss: -2.440\n",
      "Iter 91/200 - Loss: -2.641\n",
      "Iter 101/200 - Loss: -2.750\n",
      "Iter 111/200 - Loss: -2.806\n",
      "Iter 121/200 - Loss: -2.836\n",
      "Iter 131/200 - Loss: -2.854\n",
      "Iter 141/200 - Loss: -2.865\n",
      "Iter 151/200 - Loss: -2.873\n",
      "Iter 161/200 - Loss: -2.879\n",
      "Iter 171/200 - Loss: -2.884\n",
      "Iter 181/200 - Loss: -2.887\n",
      "Iter 191/200 - Loss: -2.890\n",
      "Iter 200/200 - Loss: -2.892\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 9, Fold 2:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.096\n",
      "Iter 11/200 - Loss: 0.698\n",
      "Iter 21/200 - Loss: 0.239\n",
      "Iter 31/200 - Loss: -0.255\n",
      "Iter 41/200 - Loss: -0.762\n",
      "Iter 51/200 - Loss: -1.259\n",
      "Iter 61/200 - Loss: -1.726\n",
      "Iter 71/200 - Loss: -2.136\n",
      "Iter 81/200 - Loss: -2.453\n",
      "Iter 91/200 - Loss: -2.655\n",
      "Iter 101/200 - Loss: -2.764\n",
      "Iter 111/200 - Loss: -2.820\n",
      "Iter 121/200 - Loss: -2.851\n",
      "Iter 131/200 - Loss: -2.868\n",
      "Iter 141/200 - Loss: -2.880\n",
      "Iter 151/200 - Loss: -2.888\n",
      "Iter 161/200 - Loss: -2.894\n",
      "Iter 171/200 - Loss: -2.898\n",
      "Iter 181/200 - Loss: -2.902\n",
      "Iter 191/200 - Loss: -2.905\n",
      "Iter 200/200 - Loss: -2.907\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n",
      "Using kernel2\n",
      "Sample 0, Fold 0:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 3.048\n",
      "Iter 11/200 - Loss: 1.670\n",
      "Iter 21/200 - Loss: 0.656\n",
      "Iter 31/200 - Loss: -0.103\n",
      "Iter 41/200 - Loss: -0.769\n",
      "Iter 51/200 - Loss: -1.372\n",
      "Iter 61/200 - Loss: -1.881\n",
      "Iter 71/200 - Loss: -2.250\n",
      "Iter 81/200 - Loss: -2.458\n",
      "Iter 91/200 - Loss: -2.551\n",
      "Iter 101/200 - Loss: -2.598\n",
      "Iter 111/200 - Loss: -2.626\n",
      "Iter 121/200 - Loss: -2.646\n",
      "Iter 131/200 - Loss: -2.660\n",
      "Iter 141/200 - Loss: -2.671\n",
      "Iter 151/200 - Loss: -2.681\n",
      "Iter 161/200 - Loss: -2.690\n",
      "Iter 171/200 - Loss: -2.698\n",
      "Iter 181/200 - Loss: -2.704\n",
      "Iter 191/200 - Loss: -2.711\n",
      "Iter 200/200 - Loss: -2.716\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 0, Fold 1:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.122\n",
      "Iter 11/200 - Loss: 0.704\n",
      "Iter 21/200 - Loss: 0.241\n",
      "Iter 31/200 - Loss: -0.253\n",
      "Iter 41/200 - Loss: -0.753\n",
      "Iter 51/200 - Loss: -1.240\n",
      "Iter 61/200 - Loss: -1.695\n",
      "Iter 71/200 - Loss: -2.090\n",
      "Iter 81/200 - Loss: -2.395\n",
      "Iter 91/200 - Loss: -2.594\n",
      "Iter 101/200 - Loss: -2.705\n",
      "Iter 111/200 - Loss: -2.766\n",
      "Iter 121/200 - Loss: -2.800\n",
      "Iter 131/200 - Loss: -2.821\n",
      "Iter 141/200 - Loss: -2.835\n",
      "Iter 151/200 - Loss: -2.846\n",
      "Iter 161/200 - Loss: -2.853\n",
      "Iter 171/200 - Loss: -2.860\n",
      "Iter 181/200 - Loss: -2.865\n",
      "Iter 191/200 - Loss: -2.858\n",
      "Iter 200/200 - Loss: -2.871\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 0, Fold 2:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.115\n",
      "Iter 11/200 - Loss: 0.700\n",
      "Iter 21/200 - Loss: 0.237\n",
      "Iter 31/200 - Loss: -0.260\n",
      "Iter 41/200 - Loss: -0.765\n",
      "Iter 51/200 - Loss: -1.260\n",
      "Iter 61/200 - Loss: -1.725\n",
      "Iter 71/200 - Loss: -2.133\n",
      "Iter 81/200 - Loss: -2.446\n",
      "Iter 91/200 - Loss: -2.646\n",
      "Iter 101/200 - Loss: -2.755\n",
      "Iter 111/200 - Loss: -2.811\n",
      "Iter 121/200 - Loss: -2.843\n",
      "Iter 131/200 - Loss: -2.861\n",
      "Iter 141/200 - Loss: -2.873\n",
      "Iter 151/200 - Loss: -2.880\n",
      "Iter 161/200 - Loss: -2.884\n",
      "Iter 171/200 - Loss: -2.892\n",
      "Iter 181/200 - Loss: -2.897\n",
      "Iter 191/200 - Loss: -2.900\n",
      "Iter 200/200 - Loss: -2.902\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n",
      "Sample 1, Fold 0:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.225\n",
      "Iter 11/200 - Loss: 0.788\n",
      "Iter 21/200 - Loss: 0.307\n",
      "Iter 31/200 - Loss: -0.218\n",
      "Iter 41/200 - Loss: -0.750\n",
      "Iter 51/200 - Loss: -1.266\n",
      "Iter 61/200 - Loss: -1.740\n",
      "Iter 71/200 - Loss: -2.141\n",
      "Iter 81/200 - Loss: -2.443\n",
      "Iter 91/200 - Loss: -2.626\n",
      "Iter 101/200 - Loss: -2.724\n",
      "Iter 111/200 - Loss: -2.774\n",
      "Iter 121/200 - Loss: -2.801\n",
      "Iter 131/200 - Loss: -2.818\n",
      "Iter 141/200 - Loss: -2.827\n",
      "Iter 151/200 - Loss: -2.836\n",
      "Iter 161/200 - Loss: -2.840\n",
      "Iter 171/200 - Loss: -2.845\n",
      "Iter 181/200 - Loss: -2.848\n",
      "Iter 191/200 - Loss: -2.852\n",
      "Iter 200/200 - Loss: -2.853\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 1, Fold 1:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.143\n",
      "Iter 11/200 - Loss: 0.725\n",
      "Iter 21/200 - Loss: 0.250\n",
      "Iter 31/200 - Loss: -0.258\n",
      "Iter 41/200 - Loss: -0.770\n",
      "Iter 51/200 - Loss: -1.267\n",
      "Iter 61/200 - Loss: -1.735\n",
      "Iter 71/200 - Loss: -2.143\n",
      "Iter 81/200 - Loss: -2.454\n",
      "Iter 91/200 - Loss: -2.651\n",
      "Iter 101/200 - Loss: -2.757\n",
      "Iter 111/200 - Loss: -2.811\n",
      "Iter 121/200 - Loss: -2.840\n",
      "Iter 131/200 - Loss: -2.857\n",
      "Iter 141/200 - Loss: -2.868\n",
      "Iter 151/200 - Loss: -2.872\n",
      "Iter 161/200 - Loss: -2.873\n",
      "Iter 171/200 - Loss: -2.886\n",
      "Iter 181/200 - Loss: -2.890\n",
      "Iter 191/200 - Loss: -2.894\n",
      "Iter 200/200 - Loss: -2.895\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 1, Fold 2:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.178\n",
      "Iter 11/200 - Loss: 0.770\n",
      "Iter 21/200 - Loss: 0.296\n",
      "Iter 31/200 - Loss: -0.223\n",
      "Iter 41/200 - Loss: -0.758\n",
      "Iter 51/200 - Loss: -1.277\n",
      "Iter 61/200 - Loss: -1.756\n",
      "Iter 71/200 - Loss: -2.170\n",
      "Iter 81/200 - Loss: -2.485\n",
      "Iter 91/200 - Loss: -2.678\n",
      "Iter 101/200 - Loss: -2.780\n",
      "Iter 111/200 - Loss: -2.832\n",
      "Iter 121/200 - Loss: -2.859\n",
      "Iter 131/200 - Loss: -2.875\n",
      "Iter 141/200 - Loss: -2.885\n",
      "Iter 151/200 - Loss: -2.893\n",
      "Iter 161/200 - Loss: -2.884\n",
      "Iter 171/200 - Loss: -2.903\n",
      "Iter 181/200 - Loss: -2.905\n",
      "Iter 191/200 - Loss: -2.909\n",
      "Iter 200/200 - Loss: -2.912\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n",
      "Sample 2, Fold 0:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.184\n",
      "Iter 11/200 - Loss: 0.757\n",
      "Iter 21/200 - Loss: 0.278\n",
      "Iter 31/200 - Loss: -0.241\n",
      "Iter 41/200 - Loss: -0.763\n",
      "Iter 51/200 - Loss: -1.268\n",
      "Iter 61/200 - Loss: -1.733\n",
      "Iter 71/200 - Loss: -2.135\n",
      "Iter 81/200 - Loss: -2.438\n",
      "Iter 91/200 - Loss: -2.627\n",
      "Iter 101/200 - Loss: -2.727\n",
      "Iter 111/200 - Loss: -2.778\n",
      "Iter 121/200 - Loss: -2.806\n",
      "Iter 131/200 - Loss: -2.822\n",
      "Iter 141/200 - Loss: -2.832\n",
      "Iter 151/200 - Loss: -2.840\n",
      "Iter 161/200 - Loss: -2.835\n",
      "Iter 171/200 - Loss: -2.845\n",
      "Iter 181/200 - Loss: -2.851\n",
      "Iter 191/200 - Loss: -2.856\n",
      "Iter 200/200 - Loss: -2.859\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 2, Fold 1:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.150\n",
      "Iter 11/200 - Loss: 0.722\n",
      "Iter 21/200 - Loss: 0.251\n",
      "Iter 31/200 - Loss: -0.258\n",
      "Iter 41/200 - Loss: -0.770\n",
      "Iter 51/200 - Loss: -1.269\n",
      "Iter 61/200 - Loss: -1.738\n",
      "Iter 71/200 - Loss: -2.148\n",
      "Iter 81/200 - Loss: -2.460\n",
      "Iter 91/200 - Loss: -2.656\n",
      "Iter 101/200 - Loss: -2.762\n",
      "Iter 111/200 - Loss: -2.816\n",
      "Iter 121/200 - Loss: -2.845\n",
      "Iter 131/200 - Loss: -2.861\n",
      "Iter 141/200 - Loss: -2.870\n",
      "Iter 151/200 - Loss: -2.881\n",
      "Iter 161/200 - Loss: -2.886\n",
      "Iter 171/200 - Loss: -2.891\n",
      "Iter 181/200 - Loss: -2.894\n",
      "Iter 191/200 - Loss: -2.898\n",
      "Iter 200/200 - Loss: -2.899\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 2, Fold 2:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.176\n",
      "Iter 11/200 - Loss: 0.754\n",
      "Iter 21/200 - Loss: 0.282\n",
      "Iter 31/200 - Loss: -0.232\n",
      "Iter 41/200 - Loss: -0.761\n",
      "Iter 51/200 - Loss: -1.277\n",
      "Iter 61/200 - Loss: -1.755\n",
      "Iter 71/200 - Loss: -2.169\n",
      "Iter 81/200 - Loss: -2.484\n",
      "Iter 91/200 - Loss: -2.679\n",
      "Iter 101/200 - Loss: -2.782\n",
      "Iter 111/200 - Loss: -2.834\n",
      "Iter 121/200 - Loss: -2.861\n",
      "Iter 131/200 - Loss: -2.878\n",
      "Iter 141/200 - Loss: -2.889\n",
      "Iter 151/200 - Loss: -2.896\n",
      "Iter 161/200 - Loss: -2.901\n",
      "Iter 171/200 - Loss: -2.906\n",
      "Iter 181/200 - Loss: -2.902\n",
      "Iter 191/200 - Loss: -2.889\n",
      "Iter 200/200 - Loss: -2.857\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n",
      "Sample 3, Fold 0:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.206\n",
      "Iter 11/200 - Loss: 0.804\n",
      "Iter 21/200 - Loss: 0.336\n",
      "Iter 31/200 - Loss: -0.180\n",
      "Iter 41/200 - Loss: -0.719\n",
      "Iter 51/200 - Loss: -1.252\n",
      "Iter 61/200 - Loss: -1.740\n",
      "Iter 71/200 - Loss: -2.147\n",
      "Iter 81/200 - Loss: -2.454\n",
      "Iter 91/200 - Loss: -2.639\n",
      "Iter 101/200 - Loss: -2.737\n",
      "Iter 111/200 - Loss: -2.786\n",
      "Iter 121/200 - Loss: -2.811\n",
      "Iter 131/200 - Loss: -2.827\n",
      "Iter 141/200 - Loss: -2.837\n",
      "Iter 151/200 - Loss: -2.844\n",
      "Iter 161/200 - Loss: -2.849\n",
      "Iter 171/200 - Loss: -2.845\n",
      "Iter 181/200 - Loss: -2.856\n",
      "Iter 191/200 - Loss: -2.859\n",
      "Iter 200/200 - Loss: -2.861\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 3, Fold 1:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.152\n",
      "Iter 11/200 - Loss: 0.732\n",
      "Iter 21/200 - Loss: 0.259\n",
      "Iter 31/200 - Loss: -0.251\n",
      "Iter 41/200 - Loss: -0.768\n",
      "Iter 51/200 - Loss: -1.272\n",
      "Iter 61/200 - Loss: -1.740\n",
      "Iter 71/200 - Loss: -2.152\n",
      "Iter 81/200 - Loss: -2.464\n",
      "Iter 91/200 - Loss: -2.660\n",
      "Iter 101/200 - Loss: -2.765\n",
      "Iter 111/200 - Loss: -2.819\n",
      "Iter 121/200 - Loss: -2.848\n",
      "Iter 131/200 - Loss: -2.864\n",
      "Iter 141/200 - Loss: -2.873\n",
      "Iter 151/200 - Loss: -2.880\n",
      "Iter 161/200 - Loss: -2.889\n",
      "Iter 171/200 - Loss: -2.893\n",
      "Iter 181/200 - Loss: -2.897\n",
      "Iter 191/200 - Loss: -2.899\n",
      "Iter 200/200 - Loss: -2.901\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 3, Fold 2:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.138\n",
      "Iter 11/200 - Loss: 0.726\n",
      "Iter 21/200 - Loss: 0.256\n",
      "Iter 31/200 - Loss: -0.252\n",
      "Iter 41/200 - Loss: -0.770\n",
      "Iter 51/200 - Loss: -1.273\n",
      "Iter 61/200 - Loss: -1.745\n",
      "Iter 71/200 - Loss: -2.159\n",
      "Iter 81/200 - Loss: -2.474\n",
      "Iter 91/200 - Loss: -2.673\n",
      "Iter 101/200 - Loss: -2.779\n",
      "Iter 111/200 - Loss: -2.832\n",
      "Iter 121/200 - Loss: -2.851\n",
      "Iter 131/200 - Loss: -2.874\n",
      "Iter 141/200 - Loss: -2.888\n",
      "Iter 151/200 - Loss: -2.897\n",
      "Iter 161/200 - Loss: -2.903\n",
      "Iter 171/200 - Loss: -2.908\n",
      "Iter 181/200 - Loss: -2.911\n",
      "Iter 191/200 - Loss: -2.913\n",
      "Iter 200/200 - Loss: -2.917\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n",
      "Sample 4, Fold 0:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.225\n",
      "Iter 11/200 - Loss: 0.802\n",
      "Iter 21/200 - Loss: 0.325\n",
      "Iter 31/200 - Loss: -0.191\n",
      "Iter 41/200 - Loss: -0.723\n",
      "Iter 51/200 - Loss: -1.248\n",
      "Iter 61/200 - Loss: -1.737\n",
      "Iter 71/200 - Loss: -2.151\n",
      "Iter 81/200 - Loss: -2.455\n",
      "Iter 91/200 - Loss: -2.641\n",
      "Iter 101/200 - Loss: -2.735\n",
      "Iter 111/200 - Loss: -2.787\n",
      "Iter 121/200 - Loss: -2.812\n",
      "Iter 131/200 - Loss: -2.828\n",
      "Iter 141/200 - Loss: -2.838\n",
      "Iter 151/200 - Loss: -2.844\n",
      "Iter 161/200 - Loss: -2.837\n",
      "Iter 171/200 - Loss: -2.853\n",
      "Iter 181/200 - Loss: -2.856\n",
      "Iter 191/200 - Loss: -2.860\n",
      "Iter 200/200 - Loss: -2.863\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 4, Fold 1:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.177\n",
      "Iter 11/200 - Loss: 0.750\n",
      "Iter 21/200 - Loss: 0.278\n",
      "Iter 31/200 - Loss: -0.236\n",
      "Iter 41/200 - Loss: -0.763\n",
      "Iter 51/200 - Loss: -1.275\n",
      "Iter 61/200 - Loss: -1.747\n",
      "Iter 71/200 - Loss: -2.161\n",
      "Iter 81/200 - Loss: -2.472\n",
      "Iter 91/200 - Loss: -2.666\n",
      "Iter 101/200 - Loss: -2.770\n",
      "Iter 111/200 - Loss: -2.822\n",
      "Iter 121/200 - Loss: -2.850\n",
      "Iter 131/200 - Loss: -2.866\n",
      "Iter 141/200 - Loss: -2.877\n",
      "Iter 151/200 - Loss: -2.884\n",
      "Iter 161/200 - Loss: -2.887\n",
      "Iter 171/200 - Loss: -2.892\n",
      "Iter 181/200 - Loss: -2.897\n",
      "Iter 191/200 - Loss: -2.900\n",
      "Iter 200/200 - Loss: -2.904\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 4, Fold 2:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.169\n",
      "Iter 11/200 - Loss: 0.739\n",
      "Iter 21/200 - Loss: 0.269\n",
      "Iter 31/200 - Loss: -0.242\n",
      "Iter 41/200 - Loss: -0.763\n",
      "Iter 51/200 - Loss: -1.273\n",
      "Iter 61/200 - Loss: -1.753\n",
      "Iter 71/200 - Loss: -2.167\n",
      "Iter 81/200 - Loss: -2.483\n",
      "Iter 91/200 - Loss: -2.679\n",
      "Iter 101/200 - Loss: -2.783\n",
      "Iter 111/200 - Loss: -2.836\n",
      "Iter 121/200 - Loss: -2.864\n",
      "Iter 131/200 - Loss: -2.881\n",
      "Iter 141/200 - Loss: -2.891\n",
      "Iter 151/200 - Loss: -2.888\n",
      "Iter 161/200 - Loss: -2.903\n",
      "Iter 171/200 - Loss: -2.908\n",
      "Iter 181/200 - Loss: -2.895\n",
      "Iter 191/200 - Loss: -2.909\n",
      "Iter 200/200 - Loss: -2.916\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n",
      "Sample 5, Fold 0:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.192\n",
      "Iter 11/200 - Loss: 0.771\n",
      "Iter 21/200 - Loss: 0.299\n",
      "Iter 31/200 - Loss: -0.212\n",
      "Iter 41/200 - Loss: -0.739\n",
      "Iter 51/200 - Loss: -1.258\n",
      "Iter 61/200 - Loss: -1.735\n",
      "Iter 71/200 - Loss: -2.141\n",
      "Iter 81/200 - Loss: -2.447\n",
      "Iter 91/200 - Loss: -2.635\n",
      "Iter 101/200 - Loss: -2.735\n",
      "Iter 111/200 - Loss: -2.784\n",
      "Iter 121/200 - Loss: -2.812\n",
      "Iter 131/200 - Loss: -2.828\n",
      "Iter 141/200 - Loss: -2.838\n",
      "Iter 151/200 - Loss: -2.846\n",
      "Iter 161/200 - Loss: -2.843\n",
      "Iter 171/200 - Loss: -2.851\n",
      "Iter 181/200 - Loss: -2.858\n",
      "Iter 191/200 - Loss: -2.861\n",
      "Iter 200/200 - Loss: -2.863\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 5, Fold 1:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.158\n",
      "Iter 11/200 - Loss: 0.739\n",
      "Iter 21/200 - Loss: 0.267\n",
      "Iter 31/200 - Loss: -0.241\n",
      "Iter 41/200 - Loss: -0.764\n",
      "Iter 51/200 - Loss: -1.271\n",
      "Iter 61/200 - Loss: -1.743\n",
      "Iter 71/200 - Loss: -2.156\n",
      "Iter 81/200 - Loss: -2.468\n",
      "Iter 91/200 - Loss: -2.662\n",
      "Iter 101/200 - Loss: -2.769\n",
      "Iter 111/200 - Loss: -2.821\n",
      "Iter 121/200 - Loss: -2.850\n",
      "Iter 131/200 - Loss: -2.866\n",
      "Iter 141/200 - Loss: -2.876\n",
      "Iter 151/200 - Loss: -2.878\n",
      "Iter 161/200 - Loss: -2.890\n",
      "Iter 171/200 - Loss: -2.894\n",
      "Iter 181/200 - Loss: -2.898\n",
      "Iter 191/200 - Loss: -2.901\n",
      "Iter 200/200 - Loss: -2.903\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 5, Fold 2:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.158\n",
      "Iter 11/200 - Loss: 0.744\n",
      "Iter 21/200 - Loss: 0.274\n",
      "Iter 31/200 - Loss: -0.235\n",
      "Iter 41/200 - Loss: -0.758\n",
      "Iter 51/200 - Loss: -1.272\n",
      "Iter 61/200 - Loss: -1.751\n",
      "Iter 71/200 - Loss: -2.165\n",
      "Iter 81/200 - Loss: -2.482\n",
      "Iter 91/200 - Loss: -2.679\n",
      "Iter 101/200 - Loss: -2.784\n",
      "Iter 111/200 - Loss: -2.837\n",
      "Iter 121/200 - Loss: -2.865\n",
      "Iter 131/200 - Loss: -2.879\n",
      "Iter 141/200 - Loss: -2.890\n",
      "Iter 151/200 - Loss: -2.898\n",
      "Iter 161/200 - Loss: -2.905\n",
      "Iter 171/200 - Loss: -2.909\n",
      "Iter 181/200 - Loss: -2.914\n",
      "Iter 191/200 - Loss: -2.916\n",
      "Iter 200/200 - Loss: -2.917\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n",
      "Sample 6, Fold 0:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.186\n",
      "Iter 11/200 - Loss: 0.781\n",
      "Iter 21/200 - Loss: 0.312\n",
      "Iter 31/200 - Loss: -0.198\n",
      "Iter 41/200 - Loss: -0.724\n",
      "Iter 51/200 - Loss: -1.245\n",
      "Iter 61/200 - Loss: -1.732\n",
      "Iter 71/200 - Loss: -2.136\n",
      "Iter 81/200 - Loss: -2.449\n",
      "Iter 91/200 - Loss: -2.637\n",
      "Iter 101/200 - Loss: -2.736\n",
      "Iter 111/200 - Loss: -2.787\n",
      "Iter 121/200 - Loss: -2.813\n",
      "Iter 131/200 - Loss: -2.829\n",
      "Iter 141/200 - Loss: -2.839\n",
      "Iter 151/200 - Loss: -2.846\n",
      "Iter 161/200 - Loss: -2.851\n",
      "Iter 171/200 - Loss: -2.839\n",
      "Iter 181/200 - Loss: -2.850\n",
      "Iter 191/200 - Loss: -2.859\n",
      "Iter 200/200 - Loss: -2.863\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 6, Fold 1:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.156\n",
      "Iter 11/200 - Loss: 0.738\n",
      "Iter 21/200 - Loss: 0.267\n",
      "Iter 31/200 - Loss: -0.241\n",
      "Iter 41/200 - Loss: -0.763\n",
      "Iter 51/200 - Loss: -1.272\n",
      "Iter 61/200 - Loss: -1.743\n",
      "Iter 71/200 - Loss: -2.155\n",
      "Iter 81/200 - Loss: -2.468\n",
      "Iter 91/200 - Loss: -2.664\n",
      "Iter 101/200 - Loss: -2.769\n",
      "Iter 111/200 - Loss: -2.822\n",
      "Iter 121/200 - Loss: -2.851\n",
      "Iter 131/200 - Loss: -2.864\n",
      "Iter 141/200 - Loss: -2.874\n",
      "Iter 151/200 - Loss: -2.885\n",
      "Iter 161/200 - Loss: -2.891\n",
      "Iter 171/200 - Loss: -2.895\n",
      "Iter 181/200 - Loss: -2.899\n",
      "Iter 191/200 - Loss: -2.901\n",
      "Iter 200/200 - Loss: -2.904\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 6, Fold 2:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.167\n",
      "Iter 11/200 - Loss: 0.758\n",
      "Iter 21/200 - Loss: 0.294\n",
      "Iter 31/200 - Loss: -0.215\n",
      "Iter 41/200 - Loss: -0.740\n",
      "Iter 51/200 - Loss: -1.261\n",
      "Iter 61/200 - Loss: -1.749\n",
      "Iter 71/200 - Loss: -2.169\n",
      "Iter 81/200 - Loss: -2.487\n",
      "Iter 91/200 - Loss: -2.683\n",
      "Iter 101/200 - Loss: -2.787\n",
      "Iter 111/200 - Loss: -2.838\n",
      "Iter 121/200 - Loss: -2.866\n",
      "Iter 131/200 - Loss: -2.882\n",
      "Iter 141/200 - Loss: -2.893\n",
      "Iter 151/200 - Loss: -2.900\n",
      "Iter 161/200 - Loss: -2.906\n",
      "Iter 171/200 - Loss: -2.910\n",
      "Iter 181/200 - Loss: -2.887\n",
      "Iter 191/200 - Loss: -2.916\n",
      "Iter 200/200 - Loss: -2.905\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n",
      "Sample 7, Fold 0:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.241\n",
      "Iter 11/200 - Loss: 0.831\n",
      "Iter 21/200 - Loss: 0.327\n",
      "Iter 31/200 - Loss: -0.192\n",
      "Iter 41/200 - Loss: -0.736\n",
      "Iter 51/200 - Loss: -1.260\n",
      "Iter 61/200 - Loss: -1.748\n",
      "Iter 71/200 - Loss: -2.161\n",
      "Iter 81/200 - Loss: -2.463\n",
      "Iter 91/200 - Loss: -2.648\n",
      "Iter 101/200 - Loss: -2.742\n",
      "Iter 111/200 - Loss: -2.789\n",
      "Iter 121/200 - Loss: -2.815\n",
      "Iter 131/200 - Loss: -2.831\n",
      "Iter 141/200 - Loss: -2.840\n",
      "Iter 151/200 - Loss: -2.848\n",
      "Iter 161/200 - Loss: -2.853\n",
      "Iter 171/200 - Loss: -2.857\n",
      "Iter 181/200 - Loss: -2.858\n",
      "Iter 191/200 - Loss: -2.859\n",
      "Iter 200/200 - Loss: -2.864\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 7, Fold 1:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.158\n",
      "Iter 11/200 - Loss: 0.734\n",
      "Iter 21/200 - Loss: 0.262\n",
      "Iter 31/200 - Loss: -0.250\n",
      "Iter 41/200 - Loss: -0.770\n",
      "Iter 51/200 - Loss: -1.274\n",
      "Iter 61/200 - Loss: -1.744\n",
      "Iter 71/200 - Loss: -2.156\n",
      "Iter 81/200 - Loss: -2.468\n",
      "Iter 91/200 - Loss: -2.664\n",
      "Iter 101/200 - Loss: -2.769\n",
      "Iter 111/200 - Loss: -2.822\n",
      "Iter 121/200 - Loss: -2.850\n",
      "Iter 131/200 - Loss: -2.864\n",
      "Iter 141/200 - Loss: -2.875\n",
      "Iter 151/200 - Loss: -2.885\n",
      "Iter 161/200 - Loss: -2.891\n",
      "Iter 171/200 - Loss: -2.895\n",
      "Iter 181/200 - Loss: -2.899\n",
      "Iter 191/200 - Loss: -2.902\n",
      "Iter 200/200 - Loss: -2.903\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 7, Fold 2:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.167\n",
      "Iter 11/200 - Loss: 0.737\n",
      "Iter 21/200 - Loss: 0.266\n",
      "Iter 31/200 - Loss: -0.246\n",
      "Iter 41/200 - Loss: -0.767\n",
      "Iter 51/200 - Loss: -1.277\n",
      "Iter 61/200 - Loss: -1.752\n",
      "Iter 71/200 - Loss: -2.167\n",
      "Iter 81/200 - Loss: -2.483\n",
      "Iter 91/200 - Loss: -2.679\n",
      "Iter 101/200 - Loss: -2.784\n",
      "Iter 111/200 - Loss: -2.837\n",
      "Iter 121/200 - Loss: -2.866\n",
      "Iter 131/200 - Loss: -2.876\n",
      "Iter 141/200 - Loss: -2.882\n",
      "Iter 151/200 - Loss: -2.899\n",
      "Iter 161/200 - Loss: -2.899\n",
      "Iter 171/200 - Loss: -2.873\n",
      "Iter 181/200 - Loss: -2.897\n",
      "Iter 191/200 - Loss: -2.912\n",
      "Iter 200/200 - Loss: -2.918\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n",
      "Sample 8, Fold 0:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.180\n",
      "Iter 11/200 - Loss: 0.751\n",
      "Iter 21/200 - Loss: 0.280\n",
      "Iter 31/200 - Loss: -0.228\n",
      "Iter 41/200 - Loss: -0.751\n",
      "Iter 51/200 - Loss: -1.262\n",
      "Iter 61/200 - Loss: -1.733\n",
      "Iter 71/200 - Loss: -2.137\n",
      "Iter 81/200 - Loss: -2.444\n",
      "Iter 91/200 - Loss: -2.633\n",
      "Iter 101/200 - Loss: -2.734\n",
      "Iter 111/200 - Loss: -2.784\n",
      "Iter 121/200 - Loss: -2.808\n",
      "Iter 131/200 - Loss: -2.823\n",
      "Iter 141/200 - Loss: -2.837\n",
      "Iter 151/200 - Loss: -2.845\n",
      "Iter 161/200 - Loss: -2.851\n",
      "Iter 171/200 - Loss: -2.855\n",
      "Iter 181/200 - Loss: -2.859\n",
      "Iter 191/200 - Loss: -2.861\n",
      "Iter 200/200 - Loss: -2.864\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 8, Fold 1:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.198\n",
      "Iter 11/200 - Loss: 0.788\n",
      "Iter 21/200 - Loss: 0.324\n",
      "Iter 31/200 - Loss: -0.185\n",
      "Iter 41/200 - Loss: -0.715\n",
      "Iter 51/200 - Loss: -1.243\n",
      "Iter 61/200 - Loss: -1.741\n",
      "Iter 71/200 - Loss: -2.167\n",
      "Iter 81/200 - Loss: -2.482\n",
      "Iter 91/200 - Loss: -2.677\n",
      "Iter 101/200 - Loss: -2.777\n",
      "Iter 111/200 - Loss: -2.828\n",
      "Iter 121/200 - Loss: -2.855\n",
      "Iter 131/200 - Loss: -2.870\n",
      "Iter 141/200 - Loss: -2.881\n",
      "Iter 151/200 - Loss: -2.885\n",
      "Iter 161/200 - Loss: -2.889\n",
      "Iter 171/200 - Loss: -2.892\n",
      "Iter 181/200 - Loss: -2.900\n",
      "Iter 191/200 - Loss: -2.903\n",
      "Iter 200/200 - Loss: -2.905\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 8, Fold 2:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.157\n",
      "Iter 11/200 - Loss: 0.749\n",
      "Iter 21/200 - Loss: 0.278\n",
      "Iter 31/200 - Loss: -0.233\n",
      "Iter 41/200 - Loss: -0.758\n",
      "Iter 51/200 - Loss: -1.274\n",
      "Iter 61/200 - Loss: -1.752\n",
      "Iter 71/200 - Loss: -2.168\n",
      "Iter 81/200 - Loss: -2.484\n",
      "Iter 91/200 - Loss: -2.681\n",
      "Iter 101/200 - Loss: -2.785\n",
      "Iter 111/200 - Loss: -2.838\n",
      "Iter 121/200 - Loss: -2.866\n",
      "Iter 131/200 - Loss: -2.881\n",
      "Iter 141/200 - Loss: -2.892\n",
      "Iter 151/200 - Loss: -2.901\n",
      "Iter 161/200 - Loss: -2.906\n",
      "Iter 171/200 - Loss: -2.909\n",
      "Iter 181/200 - Loss: -2.913\n",
      "Iter 191/200 - Loss: -2.916\n",
      "Iter 200/200 - Loss: -2.919\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n",
      "Sample 9, Fold 0:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.199\n",
      "Iter 11/200 - Loss: 0.768\n",
      "Iter 21/200 - Loss: 0.295\n",
      "Iter 31/200 - Loss: -0.222\n",
      "Iter 41/200 - Loss: -0.748\n",
      "Iter 51/200 - Loss: -1.264\n",
      "Iter 61/200 - Loss: -1.738\n",
      "Iter 71/200 - Loss: -2.144\n",
      "Iter 81/200 - Loss: -2.449\n",
      "Iter 91/200 - Loss: -2.637\n",
      "Iter 101/200 - Loss: -2.736\n",
      "Iter 111/200 - Loss: -2.785\n",
      "Iter 121/200 - Loss: -2.813\n",
      "Iter 131/200 - Loss: -2.828\n",
      "Iter 141/200 - Loss: -2.840\n",
      "Iter 151/200 - Loss: -2.847\n",
      "Iter 161/200 - Loss: -2.852\n",
      "Iter 171/200 - Loss: -2.855\n",
      "Iter 181/200 - Loss: -2.858\n",
      "Iter 191/200 - Loss: -2.860\n",
      "Iter 200/200 - Loss: -2.854\n",
      "Testing from 2014-01-03 00:00:00 to 2015-01-02 00:00:00\n",
      "\n",
      "Sample 9, Fold 1:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.159\n",
      "Iter 11/200 - Loss: 0.752\n",
      "Iter 21/200 - Loss: 0.281\n",
      "Iter 31/200 - Loss: -0.228\n",
      "Iter 41/200 - Loss: -0.753\n",
      "Iter 51/200 - Loss: -1.269\n",
      "Iter 61/200 - Loss: -1.745\n",
      "Iter 71/200 - Loss: -2.157\n",
      "Iter 81/200 - Loss: -2.472\n",
      "Iter 91/200 - Loss: -2.668\n",
      "Iter 101/200 - Loss: -2.771\n",
      "Iter 111/200 - Loss: -2.824\n",
      "Iter 121/200 - Loss: -2.852\n",
      "Iter 131/200 - Loss: -2.830\n",
      "Iter 141/200 - Loss: -2.875\n",
      "Iter 151/200 - Loss: -2.876\n",
      "Iter 161/200 - Loss: -2.890\n",
      "Iter 171/200 - Loss: -2.894\n",
      "Iter 181/200 - Loss: -2.899\n",
      "Iter 191/200 - Loss: -2.902\n",
      "Iter 200/200 - Loss: -2.905\n",
      "Testing from 2015-01-05 00:00:00 to 2015-12-31 00:00:00\n",
      "\n",
      "Sample 9, Fold 2:\n",
      "Training GP\n",
      "Iter 1/200 - Loss: 1.157\n",
      "Iter 11/200 - Loss: 0.751\n",
      "Iter 21/200 - Loss: 0.282\n",
      "Iter 31/200 - Loss: -0.228\n",
      "Iter 41/200 - Loss: -0.753\n",
      "Iter 51/200 - Loss: -1.270\n",
      "Iter 61/200 - Loss: -1.752\n",
      "Iter 71/200 - Loss: -2.167\n",
      "Iter 81/200 - Loss: -2.485\n",
      "Iter 91/200 - Loss: -2.681\n",
      "Iter 101/200 - Loss: -2.786\n",
      "Iter 111/200 - Loss: -2.838\n",
      "Iter 121/200 - Loss: -2.866\n",
      "Iter 131/200 - Loss: -2.883\n",
      "Iter 141/200 - Loss: -2.893\n",
      "Iter 151/200 - Loss: -2.885\n",
      "Iter 161/200 - Loss: -2.899\n",
      "Iter 171/200 - Loss: -2.908\n",
      "Iter 181/200 - Loss: -2.912\n",
      "Iter 191/200 - Loss: -2.916\n",
      "Iter 200/200 - Loss: -2.919\n",
      "Testing from 2016-01-04 00:00:00 to 2016-12-30 00:00:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_results = {}\n",
    "all_results_df = pd.DataFrame()\n",
    "for kernel_name,kernel_func_j in kernel_repo.items():\n",
    "    print (f\"Using {kernel_name}\")\n",
    "    kernel_result,kernel_models = get_kernel_cv_result_raw(kernel_func_j,10,200,True)\n",
    "    kernel_results_trials = kernel_result.loc[\"RMSE\"].reset_index().drop(columns=[\"fold\",\"test_year\"]).groupby([\"trial\",\"index\"]).mean()\n",
    "    kernel_results_trials[\"err_avg\"] = kernel_results_trials.mean(axis=1)\n",
    "    kernel_results_trials[\"rnk_\"] = kernel_results_trials[\"err_avg\"].rank()\n",
    "    all_results[kernel_name] = (kernel_results_trials,kernel_models)\n",
    "    result_i = kernel_results_trials.copy()\n",
    "    result_i = result_i.loc[result_i.rnk_==1]\n",
    "    result_i[\"kernel\"] = kernel_name\n",
    "    all_results_df = pd.concat([all_results_df,result_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bde867a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:38:57.769092Z",
     "iopub.status.busy": "2023-05-26T11:38:57.768729Z",
     "iopub.status.idle": "2023-05-26T11:38:57.781491Z",
     "shell.execute_reply": "2023-05-26T11:38:57.780687Z"
    },
    "papermill": {
     "duration": 0.105027,
     "end_time": "2023-05-26T11:38:57.783577",
     "exception": false,
     "start_time": "2023-05-26T11:38:57.678550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>2 Yr</th>\n",
       "      <th>10 Yr</th>\n",
       "      <th>err_avg</th>\n",
       "      <th>rnk_</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial</th>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <th>RMSE</th>\n",
       "      <td>0.012791</td>\n",
       "      <td>0.008976</td>\n",
       "      <td>0.010884</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 2 Yr     10 Yr   err_avg  rnk_\n",
       "trial index                                    \n",
       "7     RMSE   0.012791  0.008976  0.010884   1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_i = kernel_results_trials.copy()\n",
    "result_i = result_i.loc[result_i.rnk_==1.0]\n",
    "result_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88404cda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:38:57.963204Z",
     "iopub.status.busy": "2023-05-26T11:38:57.961911Z",
     "iopub.status.idle": "2023-05-26T11:38:57.976056Z",
     "shell.execute_reply": "2023-05-26T11:38:57.974788Z"
    },
    "papermill": {
     "duration": 0.106565,
     "end_time": "2023-05-26T11:38:57.978450",
     "exception": false,
     "start_time": "2023-05-26T11:38:57.871885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>2 Yr</th>\n",
       "      <th>10 Yr</th>\n",
       "      <th>err_avg</th>\n",
       "      <th>rnk_</th>\n",
       "      <th>kernel</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial</th>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>RMSE</th>\n",
       "      <td>0.016054</td>\n",
       "      <td>0.010183</td>\n",
       "      <td>0.013118</td>\n",
       "      <td>1.0</td>\n",
       "      <td>kernel1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <th>RMSE</th>\n",
       "      <td>0.012791</td>\n",
       "      <td>0.008976</td>\n",
       "      <td>0.010884</td>\n",
       "      <td>1.0</td>\n",
       "      <td>kernel2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 2 Yr     10 Yr   err_avg  rnk_   kernel\n",
       "trial index                                             \n",
       "4     RMSE   0.016054  0.010183  0.013118   1.0  kernel1\n",
       "7     RMSE   0.012791  0.008976  0.010884   1.0  kernel2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b4f7e32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:38:58.158886Z",
     "iopub.status.busy": "2023-05-26T11:38:58.157952Z",
     "iopub.status.idle": "2023-05-26T11:38:58.170235Z",
     "shell.execute_reply": "2023-05-26T11:38:58.169167Z"
    },
    "papermill": {
     "duration": 0.105684,
     "end_time": "2023-05-26T11:38:58.172717",
     "exception": false,
     "start_time": "2023-05-26T11:38:58.067033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_results_df.to_csv(\"all_results_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25df1180",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:38:58.354908Z",
     "iopub.status.busy": "2023-05-26T11:38:58.354508Z",
     "iopub.status.idle": "2023-05-26T11:38:58.358448Z",
     "shell.execute_reply": "2023-05-26T11:38:58.357564Z"
    },
    "papermill": {
     "duration": 0.096619,
     "end_time": "2023-05-26T11:38:58.360333",
     "exception": false,
     "start_time": "2023-05-26T11:38:58.263714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('all_results.pickle', 'wb') as handle:\n",
    "#     pickle.dump(all_results, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592a868b",
   "metadata": {
    "papermill": {
     "duration": 0.088842,
     "end_time": "2023-05-26T11:38:58.538826",
     "exception": false,
     "start_time": "2023-05-26T11:38:58.449984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7d902",
   "metadata": {
    "papermill": {
     "duration": 0.09023,
     "end_time": "2023-05-26T11:38:58.718496",
     "exception": false,
     "start_time": "2023-05-26T11:38:58.628266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c7e618",
   "metadata": {
    "papermill": {
     "duration": 0.0888,
     "end_time": "2023-05-26T11:38:58.896050",
     "exception": false,
     "start_time": "2023-05-26T11:38:58.807250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd88ac6",
   "metadata": {
    "papermill": {
     "duration": 0.089776,
     "end_time": "2023-05-26T11:38:59.074205",
     "exception": false,
     "start_time": "2023-05-26T11:38:58.984429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8449.173809,
   "end_time": "2023-05-26T11:39:02.335472",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-26T09:18:13.161663",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
